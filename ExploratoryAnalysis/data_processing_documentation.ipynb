{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "This notebook contains notes and documentation on data processing that was used to prepare the data for our testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_processing_modules as dpm\n",
    "# for MIND_large data processing change the folder name in all associated functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Processing\n",
    "Initially the data was stored in .tsv format without column headers. Via data_to_csv in data_processing_modules we were able to change it into a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changes to csv format\n",
    "# Behaviors and then news\n",
    "dpm.data_to_csv(True, '../MIND_large/tsv/behaviors.tsv')\n",
    "dpm.data_to_csv(False, '../MIND_large/tsv/news.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing for popularity counts \n",
    "To access popularity counts for both categories and articles, we made create_popularity_dfs and create_popularity_csvs to extract popularity information and output it into a csv for later use in visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behaviors = pd.read_csv('../MIND_large/csv/behaviors.csv', index_col=0)\n",
    "news = pd.read_csv('../MIND_large/csv/news.csv', index_col=0)\n",
    "# dpm.create_popularity_csvs(news, behaviors, small=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = behaviors[behaviors['history'].isna() == False] \n",
    "behaviors.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow compatibility\n",
    "Tensorflow recommenders requires the dataset to be in a specific format in order for it to be compatible with its systems. Using decompose_interactions we are able to create a dataframe that is tensorflow compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 5000000 # Update to determine size of data used in decompose interactions\n",
    "tf_dataset = dpm.decompose_interactions(num_rows, news, behaviors)\n",
    "tf_dataset.to_csv('../MIND_large/csv/tensorflowDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = pd.read_csv('../MIND_large/csv/tensorflowDataset.csv')\n",
    "tf_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Processing\n",
    "Due to the inclusion of the interaction timestamp in the behaviors data we analyzed the popularity of articles at different times of day. To process this data we used create_interaction_counts (behaviors_with_individual_counts). Subsequently we used modify_hourly which extracts the hour from the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = ['popularity_type', 'lifestyle', 'health', 'news', 'sports', 'weather', 'entertainment', 'foodanddrink', 'autos', 'travel', 'video', 'tv', 'finance', 'movies', 'music', 'kids', 'middleeast', 'games']\n",
    "pop[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['user_id', 'history'] + [category + '_history' for category in news['category'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpm.create_interaction_counts()\n",
    "behaviors = pd.read_csv('../MIND_large/csv/behaviors_with_individual_counts.csv', index_col=0).drop(columns='Unnamed: 0')\n",
    "behaviors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_points = pd.date_range(start='2019-11-09 00:00:00', end='2019-11-15 00:00:00', freq='h') # hourly ranges for the time of the behaviors dataset\n",
    "# going to want to adjust cutpoints so that we are specifically thinking of hours from 1 - 24 with 24 being midnight (0)\n",
    "\n",
    "# Create labels for the bins.\n",
    "bins_str = cut_points.astype(str).values\n",
    "labels = ['({}, {}]'.format(bins_str[i-1], bins_str[i]) for i in range(1, len(bins_str))]\n",
    "\n",
    "# Apply the bins to the time column.\n",
    "behaviors['hour'] = pd.cut(behaviors['time'], cut_points, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = dpm.modify_hourly(behaviors)\n",
    "behaviors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_lists(df):\n",
    "    # Initialize a dictionary to store your aggregated lists\n",
    "    agg_dict = {}\n",
    "    # Loop through each category to aggregate '_history' and '_impression'\n",
    "    for category in news['category'].unique():\n",
    "        # agg_dict[f'{category}_history'] = list(df[f'{category}_history'])\n",
    "        agg_dict[f'{category}_impression'] = list(df[f'{category}_impression'])\n",
    "    return pd.Series(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = pd.read_csv('../MIND_large/csv/behaviors_with_individual_counts.csv', index_col=0).drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors['history'] = behaviors['history'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_impressions_df = behaviors.groupby(['user_id', 'history'] + [category + '_history' for category in news['category'].unique()])['impressions'].apply(list).reset_index()\n",
    "user_impressions_df.set_index('user_id', inplace=True)\n",
    "user_impressions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_impression_preference = behaviors.groupby('user_id')[[category + '_impression' for category in news['category'].unique()]].sum().reset_index()\n",
    "user_impression_preference.set_index('user_id', inplace=True)\n",
    "user_impression_preference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_df1 = set(user_impressions_df.index)\n",
    "indices_df2 = set(user_impression_preference.index)\n",
    "\n",
    "# Find indices in df1 not in df2\n",
    "unique_to_df1 = indices_df1 - indices_df2\n",
    "\n",
    "# Find indices in df2 not in df1\n",
    "unique_to_df2 = indices_df2 - indices_df1\n",
    "\n",
    "# Optionally, convert these sets back to lists if you need list outputs\n",
    "unique_to_df1_list = list(unique_to_df1)\n",
    "unique_to_df2_list = list(unique_to_df2)\n",
    "\n",
    "# Print or use the unique indices as needed\n",
    "print(\"Indices in df1 not in df2:\", unique_to_df1_list)\n",
    "print(\"Indices in df2 not in df1:\", unique_to_df2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_impressions_df.to_csv('../MIND_large/csv/behaviors_grouped_with_history.csv')\n",
    "user_impression_preference.to_csv('../MIND_large/csv/behaviors_grouped_with_impression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_impressions_df.reset_index(inplace=True)\n",
    "user_impression_preference.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_impressions_df.drop(columns='index',inplace=True)\n",
    "# user_impression_preference.drop(columns='index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_processing_modules as dpm\n",
    "user_impressions_df = pd.read_csv('../MIND_large/csv/behaviors_grouped_with_history.csv').reset_index()\n",
    "user_impression_preference = pd.read_csv('../MIND_large/csv/behaviors_grouped_with_history.csv').reset_index()\n",
    "feature_matrix = user_impressions_df.merge(user_impression_preference)\n",
    "feature_matrix\n",
    "user_interacted = feature_matrix[['user_id', 'history', 'impressions']]\n",
    "user_interacted.head() \n",
    "del feature_matrix\n",
    "del user_impressions_df\n",
    "del user_impression_preference\n",
    "news = pd.read_csv('../MIND_large/csv/news.csv')['news_id']\n",
    "news_data = {news_id : np.full(255990, -1, dtype='int8') for news_id in news}\n",
    "def populate_dictionaries(behaviors_frame):\n",
    "    \"\"\"\n",
    "    Populates the news data dictionary with user preferences where each user_id corresponds to a row index and the columns correspond to news articles.\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    # Might just want to use the popularity counts that are already found in behaviors with popularity counts? that could be a lot better imho\n",
    "    for history, impressions in zip(behaviors_frame['history'], behaviors_frame['impressions']):\n",
    "\n",
    "        if history != '-1':\n",
    "            for news_id in history.split():\n",
    "                meep = news_data[news_id]\n",
    "                meep[index] = 1\n",
    "\n",
    "        if type(impressions) != float:    \n",
    "            \n",
    "            impressions = impressions.replace('[', '')\n",
    "            impressions = impressions.replace(']', '')\n",
    "\n",
    "            for impression_string in impressions.split(','):\n",
    "                impression_string = impression_string.replace(\"'\", \"\")\n",
    "                for impression in impression_string.split():\n",
    "                \n",
    "                    impression_info = dpm.clean_impression(impression)\n",
    "                    if impression_info['score'] == '1':\n",
    "                        news_data[impression_info['article_ID']][index] = 1\n",
    "                    else:\n",
    "                        news_data[impression_info['article_ID']][index] = 0\n",
    "        index += 1\n",
    "\n",
    "populate_dictionaries(user_interacted)\n",
    "del user_interacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "72023 / 7\n",
    "matrix_separators = [10289 * i for i in range(1, 8)]\n",
    "matrix_separators.insert(0, 0)\n",
    "matrix_separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(matrix_separators)-1):\n",
    "    start, end = matrix_separators[index], matrix_separators[index+1]\n",
    "    user_item_chunk = np.empty((255990, 10290), dtype='int8')\n",
    "    for index in range(start, end):\n",
    "        key = news[index]\n",
    "        user_item_chunk[:, index] = news_data[key]\n",
    "        del news_data[key]\n",
    "    np.save(f'../MIND_large/{index+1}user_item_mat.npy', user_item_chunk)\n",
    "    del user_item_chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1]], dtype=int8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [10289, 20578, 30867, 41156, 51445, 61734, 72023]\n",
    "\n",
    "matrix = np.load(f'../MIND_large/{indices[0]}user_item_mat.npy')\n",
    "for index in indices[1:4]:\n",
    "    matrix = np.append(matrix, np.load(f'../MIND_large/{index}user_item_mat.npy'), axis=1)\n",
    "\n",
    "matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51445\n",
      "61734\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for index in indices[4:]:\n",
    "    print(index)\n",
    "    matrix = np.append(matrix, np.load(f'../MIND_large/{index}user_item_mat.npy'), axis=1)\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(matrix_separators)-1):\n",
    "    start, end = matrix_separators[index], matrix_separators[index+1]\n",
    "user_item_matrix = np.empty((255990, 72023), dtype='int8')\n",
    "\n",
    "# Assuming you have a list or iterable of arrays in news_data.values()\n",
    "\n",
    "for index in range(72023): \n",
    "    key = news[index]\n",
    "    user_item_matrix[:, index] = news_data[key]\n",
    "    del news_data[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = np.column_stack(list(news_data.values()))\n",
    "np.save('../MIND_large/user_item_mat.npy', user_item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = user_interacted['user_id'].unique()\n",
    "user_item_matrix = pd.DataFrame(data=news_data, index=user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Processing\n",
    "With the goal of minimizing search spaces and making our recommenders more efficient we utilized clustering. During the clustering of news articles we extracted embeddings from a pre-trained BERT model and applied them to the abstracts and titles present in the dataset with create_text_embeddings. In addition to using BERT embeddings, we used scikit learn's bag of words and tf-idf vectorizers. Utilizing scikit-learn vectorizers requires only a few lines of code, therefore any preprocessing is done during clustering instead of prior like BERT embeddings below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpm.preprocess_BERT_embeddings(news, small=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might want to consider using UMAP union on the title and abstract embeddings since currently UMAP is\n",
    "# reducing all of them together which could cause a loss of data quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recSysEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
