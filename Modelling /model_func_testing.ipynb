{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS and SDG\n",
    "### Main Steps\n",
    "* Initialize $U$ and $V$\n",
    "* Determine accuracy metrics\n",
    "* Set up max distance calculation\n",
    "* Load in matrix \n",
    "* Compute solutions to $U$ and $V$\n",
    "* Monitor Optimization Progress\n",
    "* Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing U and V\n",
    "Random initialization for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 # five latent factors tentatively \n",
    "np.random.seed(42)\n",
    "# I is the number of users <- these will be determined by cluster size\n",
    "# M is the number of items \n",
    "# Can either fill with 1's or random values\n",
    "U = np.random.uniform(0, 1, size=K*I).reshape((I, K))\n",
    "V = np.random.uniform(0, 1, size=K*M).reshape((M, K))\n",
    "Uold = np.zeros_like(U)\n",
    "Vold = np.zeros_like(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy metrics\n",
    "Choosing RMSE for simplicity as well as setting up max update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(X, Y):\n",
    "    return np.sqrt(np.nanmean((X-Y)**2))\n",
    "\n",
    "def max_update(X, Y):\n",
    "    return np.noram(((X-Y)/Y).ravel(), np.inf)\n",
    "\n",
    "error = [(0, rmse(R, np.inner(U, V)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD function\n",
    "For the stochastic gradient descent function, we make update formulae for the U and V respectively under SDG.\n",
    "\n",
    "Within SGD the update functions for vectors in the user and item matrices are as follows: \n",
    "\n",
    "For all $m, i \\in R$ where $R_{m,i}$ is an observed rating and $\\alpha$ is the rate parameter,\n",
    "\n",
    "$U_{i} = U_{i} + \\alpha V_{m}(R_{m,i} - <V_{m}, U_{i}>)$\n",
    "\n",
    "$V_{m} = V_{m} + \\alpha U_{i}(R_{m,i} - <V_{m}, U_{i}>)$\n",
    "\n",
    "Within the error function we pass in the inner product of $V$ and $U$, $\\hat{R}$, along with ratings matrix $R$ to calculate the RMSE. RMSE was calculated as follows:\n",
    "\n",
    "For vectors $x_i \\in R$, $y_i \\in \\hat{R}$, $\\text{RMSE}(R, \\hat{R}) =  \\left[\\frac{1}{n}\\sum_{i=1}^{n} \\|x_i - y_i\\|_2^2 \\right]^{1/2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_SGD(U, V, R, error, update, rate=0.1, max_iterations=300, threshold=0.001):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent on user and item matrices U and V optimizing the RMSE. \n",
    "    \"\"\"\n",
    "    # Optimize over 300 iterations\n",
    "    for iteration in range(1, max_iterations): # starting from one due to first iteration being hardcoded\n",
    "        for m, i in zip(*np.where(~np.isnan(R))): # might want to change to not is nan but is zero as that is where no ratings are\n",
    "            U[i] = U[i] + rate*V[m]*(R.iloc[m,i] - np.inner(V[m], U[i]))\n",
    "            V[m] = V[m] + rate*U[i]*(R.iloc[m,i] - np.inner(V[m], U[i]))\n",
    "        error += [(t, rmse(R, np.inner(V,U)))]\n",
    "    return U, V, error\n",
    "\n",
    "\n",
    "error = pd.DataFrame(error, columns=['iteration', 'rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS function\n",
    "For the alternating least squares function we make some slight adjustments, instead of updating each vector we update separate feature matrices $U$ $V$ and handle regularization terms later on, updates are as follows:\n",
    "\n",
    "Load in U and V initalized to all 1s\n",
    "Create 2 hashmaps, 1 containing user index as keys, and indices to articles they've rated as values. The other containing article index as keys, and index to users that have rated them as values.\n",
    "\n",
    "To create the hashmaps, when we load in our matrix we have to do a larger calculation iterating through all the rows, and then for every row going through the columns finding their values. OR we can just use a sparse matrix which means a lot less iteration during the algorithm and more up front. \n",
    "\n",
    "Then next steps would be to make sparse representation of the matrix (there are functions for this in scipy, or we can do it ourselves)\n",
    "\n",
    "Once we have this we then do the fixing of a matrix and the optimization of the others\n",
    "\n",
    "for i in 1 to n/m:\n",
    "* U[i] = (sum: V[hash index] * V[hash index]^transpose)^-1 * R[indices in V that correspond to items that User i has rated] * V[hash index]\n",
    "* V[j] = (sum: U[hash index] * U[hash index]^transpose)^-1 * R[indices in U that correspond to users that rated item j] * U[hash index]\n",
    "\n",
    "Highlight then is that we can just use the matrices like above, with indices related to the user, now we could also do some up front hashing with a sparse representation of the matrix, or we could also just see if the sparse matrix works that way?\n",
    "\n",
    "\n",
    "Ah it makes sense now, yi yi^t is a column vector times a row, or n x 1 times 1 x n, which is therefore an NxN matrix, which when subtracted normalization lambda identity makes so much more sense\n",
    "The interesting thing then becomes, can I just multiply the sub-matrix of V times its transpose and get the same result?\n",
    "\n",
    "We need a matrix LOL just using a hash table is a good way to go, lets make some psuedocode below to get a better idea of how we could make a good hashtable \n",
    "\n",
    "We'd want two hashtables:\n",
    "\n",
    "user table, needs to store for every user the items that they interacted with as index and then we access scores with indexing on the matrix and we could consider using touples as well to keep their associated score\n",
    "item table, needs to store all users for every item that interacted with it as well as their ratings\n",
    "\n",
    "these tables should be local so that they arent too memory hungry.\n",
    "\n",
    "For this we would just have to activate a for loop and iterate over stuff with the zip function filling out ratings similarly to the other matrix functions that are already established. And would we want to do this with the data frame while we compose R? That way we can compose R V and U at the same time? We could also just nested forloop that bitch, or use np.flatten on the matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix function testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_big_boy():\n",
    "    full = pd.DataFrame()\n",
    "    for i in range(4):\n",
    "        df = pd.read_csv(f\"../MIND_large/csv/tensorflow_dataset_chunk{i}.csv\", index_col=0)\n",
    "        full = pd.concat([full, df])\n",
    "    return full\n",
    "full_tf = create_big_boy()\n",
    "news_text = pd.read_csv('../MIND_large/csv/news_cluster_labels.csv')\n",
    "all_ratings2 = full_tf.groupby('user_id')['news_id'].apply(list).reset_index()\n",
    "scores = full_tf.groupby('user_id')['score'].apply(list).reset_index()\n",
    "all_ratings2['scores'] = scores['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create our matrices while respecting the need for hash maps for the ALS algorithm \n",
    "We need a map of 'item' index and user indexes to optimize V\n",
    "We need a map of 'user' index and item indexes to optimize U \n",
    "When we create the item cluster matrix we are taking the dataset as well as the number of users or items depending on our clustering method\n",
    "\n",
    "So generally speaking for our map building, \n",
    "For item index and the user indices, we initialize a map of all 'items' as indices, and since were quantifying users by their index after grouping them together we can just append them to a list at the item index whenever they show up\n",
    "For user index and item indices, in the for loop all we have to do is check if the user is there and if not we add them but if so we add their item index, which for item clustering is just their cluster, under user clustering with normal items we would have to find each items index and then after having each items index we would need to see when a cluster rates something we'd attach that index to their list, and then add that cluster to the item index list in the other table, so tbh lots of hash maps wow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_split = '80_20'):\n",
    "    \"\"\"\n",
    "    Loads in the full training dataset predicated upon the train test split specified. \n",
    "    \"\"\"\n",
    "    full = pd.DataFrame()\n",
    "    for i in range(2):\n",
    "        df = pd.read_csv(f\"../MIND_large/{train_split}/train_chunk{i}.csv\", index_col=0)\n",
    "        full = pd.concat([full, df])   \n",
    "    news_text = pd.read_csv('../MIND_large/csv/news_cluster_labels.csv')\n",
    "    all_ratings = full.groupby('user_id')['news_id'].apply(list).reset_index()\n",
    "    scores = full.groupby('user_id')['score'].apply(list).reset_index()\n",
    "    all_ratings['scores'] = scores['score']\n",
    "    return all_ratings, news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_item_cluster_mat(ratings_df, num_users = 255990, num_clusters = 30, isALS=False):\n",
    "    \"\"\"\n",
    "    Creates a user item ratings matrix R with item clusters and potentially associated hash maps for use within ALS matrix factorization. Due to the nature of ALS, the \n",
    "    item indices that a user has interacted and the user indices that have interacted with an item get used heavily so these hash maps\n",
    "    are initialized during construction of R. This is a modified version of a similar function to deal with item clusters.\n",
    "\n",
    "    Args:\n",
    "        ratings_df (pd.DataFrame) : A pandas dataframe containing the result of grouping the dataset by userID, and applying\n",
    "            lists to both the articles they interacted with and their scores.\n",
    "        num_users (int) : The number of users that are being used for the matrix, gets used to determine the number of rows\n",
    "            necessary for the hash table that is used to make the matrix. Also gets used to generate ALS specific hash tables\n",
    "            for efficient subsetting of the ratings, item and user feature matrices.\n",
    "        num_clusters (int) : The number of clusters created for the items to be used in generating the number of columns \n",
    "            necessary for the hash table that is used to make the matrix. Also gets used to generate ALS specific hash tables\n",
    "            for efficient subsetting of the ratings, item and user feature matrices.\n",
    "        isALS (bool) : A boolean to determine if hash tables to store extra values should be generated for the ALS matrix factorization algorithm.\n",
    "    \n",
    "    Returns:\n",
    "        If isALS is True,\n",
    "        np.column_stack(list(matrix.values())), cluster_idx, user_idx (np.2darray, dict, dict) : The ratings matrix and resulting ALS specific hash tables for \n",
    "            column : row values and row : column values if isALS is true. \n",
    "        If isALS is False,\n",
    "        np.column_stack(list(matrix.values())) (np.2darray) : The ratings matrix.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialize the hash map that will create the matrix as a list of np zero arrays for each cluster and the hashmap of item clusters .\n",
    "    item_clusters = {item : cluster for item, cluster in zip(news_text['news_id'], news_text['labels'])}\n",
    "    matrix = {cluster : np.full(num_users, 0, dtype='int8') for cluster in range(num_clusters)} \n",
    "    \n",
    "    if isALS:\n",
    "        # Initialize the cluster hashmap, which is used to track an item index and all row indices that engage with that item.\n",
    "        cluster_idx = {cluster : set() for cluster in range(num_clusters)}\n",
    "\n",
    "        # Initialize the user hashmap, which is used to track a user index and all item indices that the user engaged with.\n",
    "        user_idx = {user_id : set() for user_id in range(num_users)}\n",
    "        \n",
    "    # Initialize a counter to keep track of user index.\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate over every user, their ratings and score in the rating matrix .\n",
    "    for user, ratings, score in zip(ratings_df['user_id'], ratings_df['news_id'], ratings_df['scores']):\n",
    "        for index in range(len(ratings)):\n",
    "            # Get the news id of the interaction and their rating.\n",
    "            news_id = ratings[index]\n",
    "            num = score[index]\n",
    "\n",
    "            # If the rating is not zero, add a 1 to the cluster of the article.\n",
    "            if num != 0:    \n",
    "                matrix[item_clusters[news_id]][counter] += 1\n",
    "                \n",
    "                # If we are using ALS, add relevant indices to the hashmaps.\n",
    "                if isALS:\n",
    "                    cluster_idx[item_clusters[news_id]].add(counter)\n",
    "                    user_idx[counter].add(item_clusters[news_id])\n",
    "        counter += 1\n",
    "    \n",
    "    # Return the full matrix and arrays if we are using ALS, otherwise return only the matrix\n",
    "    return np.column_stack(list(matrix.values())), cluster_idx, user_idx if isALS else np.column_stack(list(matrix.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "item_idx = {}\n",
    "def create_user_cluster_mat(rating_mat, num_user_clusters=10, isALS=False):\n",
    "    \"\"\"\n",
    "    Creates a user item ratings matrix R with user clusters and potentially associated hash maps for use within ALS matrix factorization. Due to the nature of ALS, the \n",
    "    item indices that a user has interacted and the user indices that have interacted with an item get used heavily so these hash maps\n",
    "    are initialized during construction of R. This is a modified version of a similar function to deal with user clusters.\n",
    "\n",
    "    Args:\n",
    "        ratings_df (pd.DataFrame) : A pandas dataframe containing the result of grouping the dataset by userID, and applying\n",
    "            lists to both the articles they interacted with and their scores.\n",
    "        num_users (int) : The number of users that are being used for the matrix, gets used to determine the number of rows\n",
    "            necessary for the hash table that is used to make the matrix. Also gets used to generate ALS specific hash tables\n",
    "            for efficient subsetting of the ratings, item and user feature matrices.\n",
    "        num_clusters (int) : The number of clusters created for the items to be used in generating the number of columns \n",
    "            necessary for the hash table that is used to make the matrix. Also gets used to generate ALS specific hash tables\n",
    "            for efficient subsetting of the ratings, item and user feature matrices.\n",
    "        isALS (bool) : A boolean to determine if hash tables to store extra values should be generated for the ALS matrix factorization algorithm.\n",
    "    \n",
    "    Returns:\n",
    "        If isALS is True,\n",
    "        np.column_stack(list(matrix.values())), cluster_idx, item_idx (np.2darray, dict, dict) : The ratings matrix and resulting ALS specific hash tables for \n",
    "            column : row values and row : column values if isALS is true. \n",
    "        If isALS is False,\n",
    "        np.column_stack(list(matrix.values())) (np.2darray) : The ratings matrix.\n",
    "    \"\"\"\n",
    "    # Initialize the hash map that will create the matrix as a list of np zero arrays for each news_id and the hashmap of user clusters.\n",
    "    matrix = {news_id : np.full(num_user_clusters, 0, dtype='int8') for news_id in news_text['news_id']}\n",
    "    user_clusters= {user : cluster for user, cluster in zip(user_clustered['user_id'], user_clustered['cluster'])}\n",
    "    \n",
    "    if isALS:\n",
    "        # Initialize the item idx hash map to create a hash map that can be used to check all row indices that have an appearance in the column.\n",
    "        item_lookup = {news_id : index for index, news_id in enumerate(news_text['news_id'])}\n",
    "        item_idx = {index : set() for index in range(len(news_text['news_id']))}\n",
    "\n",
    "        # Initialize the cluster idx hash map to create a hash map that can be used to check all column indicies that have an appearance in the row.\n",
    "        cluster_idx = {cluster : set() for cluster in range(num_user_clusters)}\n",
    "\n",
    "    \n",
    "    # Iterate over the user ids, ratings and scores.\n",
    "    for user, ratings, score in zip(rating_mat['user_id'], rating_mat['news_id'], rating_mat['scores']):\n",
    "        \n",
    "        # Determine the users cluster and then iterate all of their ratings\n",
    "        cluster = user_clusters[user]\n",
    "        for index in range(len(ratings)):\n",
    "            # Get the news ID and score of their rating.\n",
    "            news_id = ratings[index]\n",
    "            num = score[index]\n",
    "\n",
    "            # If the score is not zero, add 1 to the clusters score for that item\n",
    "            if num != 0:\n",
    "                matrix[news_id][cluster] += 1  # Access the column of the matrix, and then find the index in that column for that users cluster, then increment by 1\n",
    "                \n",
    "                # If we are using ALS, add the relevant indices to the hash map.\n",
    "                if isALS:\n",
    "                    # Add the corresponding cluster number to the column index for the news id.\n",
    "                    item_idx[item_lookup[news_id]].add(cluster)\n",
    "\n",
    "                    # Add the column index to the clusters key.\n",
    "                    cluster_idx[cluster].add(item_lookup[news_id])\n",
    "        \n",
    "    return np.column_stack(list(matrix.values())), item_idx,  cluster_idx if isALS else np.column_stack(list(matrix.values()))\n",
    "    \n",
    "matrix = create_item_cluster_mat(all_ratings2)\n",
    "np.save('../MIND_large/csv/user_cluster_mat.npy', matrix)\n",
    "# maybe we want to make a lookup table object that has high functionality, where we can add user and item maps, pull indices from the initialized object, etc? later / icing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
