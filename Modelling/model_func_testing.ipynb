{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS and SDG\n",
    "### Main Steps\n",
    "* Initialize $U$ and $V$\n",
    "* Determine accuracy metrics\n",
    "* Set up max distance calculation\n",
    "* Load in matrix \n",
    "* Compute solutions to $U$ and $V$\n",
    "* Monitor Optimization Progress\n",
    "* Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing U and V\n",
    "Random initialization for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 # five latent factors tentatively \n",
    "np.random.seed(42)\n",
    "# I is the number of users <- these will be determined by cluster size\n",
    "# M is the number of items \n",
    "# Can either fill with 1's or random values\n",
    "U = np.random.uniform(0, 1, size=K*I).reshape((I, K))\n",
    "V = np.random.uniform(0, 1, size=K*M).reshape((M, K))\n",
    "Uold = np.zeros_like(U)\n",
    "Vold = np.zeros_like(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy metrics\n",
    "Choosing RMSE for simplicity as well as setting up max update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(X, Y):\n",
    "    return np.sqrt(np.nanmean((X-Y)**2))\n",
    "\n",
    "def max_update(X, Y):\n",
    "    return np.noram(((X-Y)/Y).ravel(), np.inf)\n",
    "\n",
    "error = [(0, rmse(R, np.inner(U, V)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD function\n",
    "For the stochastic gradient descent function, we make update formulae for the U and V respectively under SDG.\n",
    "\n",
    "Within SGD the update functions for vectors in the user and item matrices are as follows: \n",
    "\n",
    "For all $m, i \\in R$ where $R_{m,i}$ is an observed rating and $\\alpha$ is the rate parameter,\n",
    "\n",
    "$U_{i} = U_{i} + \\alpha V_{m}(R_{m,i} - <V_{m}, U_{i}>)$\n",
    "\n",
    "$V_{m} = V_{m} + \\alpha U_{i}(R_{m,i} - <V_{m}, U_{i}>)$\n",
    "\n",
    "Within the error function we pass in the inner product of $V$ and $U$, $\\hat{R}$, along with ratings matrix $R$ to calculate the RMSE. RMSE was calculated as follows:\n",
    "\n",
    "For vectors $x_i \\in R$, $y_i \\in \\hat{R}$, $\\text{RMSE}(R, \\hat{R}) =  \\left[\\frac{1}{n}\\sum_{i=1}^{n} \\|x_i - y_i\\|_2^2 \\right]^{1/2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_SGD(U, V, R, error, update, rate=0.1, max_iterations=300, threshold=0.001):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent on user and item matrices U and V optimizing the RMSE. \n",
    "    \"\"\"\n",
    "    # Optimize over 300 iterations\n",
    "    for iteration in range(1, max_iterations): # starting from one due to first iteration being hardcoded\n",
    "        for m, i in zip(*np.where(~np.isnan(R))): # might want to change to not is nan but is zero as that is where no ratings are\n",
    "            U[i] = U[i] + rate*V[m]*(R.iloc[m,i] - np.inner(V[m], U[i]))\n",
    "            V[m] = V[m] + rate*U[i]*(R.iloc[m,i] - np.inner(V[m], U[i]))\n",
    "        error += [(t, rmse(R, np.inner(V,U)))]\n",
    "    return U, V, error\n",
    "\n",
    "\n",
    "error = pd.DataFrame(error, columns=['iteration', 'rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS function\n",
    "For the alternating least squares function we make some slight adjustments, instead of updating each vector we update separate feature matrices $U$ $V$ and handle regularization terms later on, updates are as follows:\n",
    "\n",
    "Load in U and V initalized to all 1s\n",
    "Create 2 hashmaps, 1 containing user index as keys, and indices to articles they've rated as values. The other containing article index as keys, and index to users that have rated them as values.\n",
    "\n",
    "To create the hashmaps, when we load in our matrix we have to do a larger calculation iterating through all the rows, and then for every row going through the columns finding their values. OR we can just use a sparse matrix which means a lot less iteration during the algorithm and more up front. \n",
    "\n",
    "Then next steps would be to make sparse representation of the matrix (there are functions for this in scipy, or we can do it ourselves)\n",
    "\n",
    "Once we have this we then do the fixing of a matrix and the optimization of the others\n",
    "\n",
    "for i in 1 to n/m:\n",
    "* U[i] = (sum: V[hash index] * V[hash index]^transpose)^-1 * R[indices in V that correspond to items that User i has rated] * V[hash index]\n",
    "* V[j] = (sum: U[hash index] * U[hash index]^transpose)^-1 * R[indices in U that correspond to users that rated item j] * U[hash index]\n",
    "\n",
    "Highlight then is that we can just use the matrices like above, with indices related to the user, now we could also do some up front hashing with a sparse representation of the matrix, or we could also just see if the sparse matrix works that way?\n",
    "\n",
    "\n",
    "Ah it makes sense now, yi yi^t is a column vector times a row, or n x 1 times 1 x n, which is therefore an NxN matrix, which when subtracted normalization lambda identity makes so much more sense\n",
    "The interesting thing then becomes, can I just multiply the sub-matrix of V times its transpose and get the same result?\n",
    "\n",
    "We need a matrix LOL just using a hash table is a good way to go, lets make some psuedocode below to get a better idea of how we could make a good hashtable \n",
    "\n",
    "We'd want two hashtables:\n",
    "\n",
    "user table, needs to store for every user the items that they interacted with as index and then we access scores with indexing on the matrix and we could consider using touples as well to keep their associated score\n",
    "item table, needs to store all users for every item that interacted with it as well as their ratings\n",
    "\n",
    "these tables should be local so that they arent too memory hungry.\n",
    "\n",
    "For this we would just have to activate a for loop and iterate over stuff with the zip function filling out ratings similarly to the other matrix functions that are already established. And would we want to do this with the data frame while we compose R? That way we can compose R V and U at the same time? We could also just nested forloop that bitch, or use np.flatten on the matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix function testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_big_boy():\n",
    "    full = pd.DataFrame()\n",
    "    for i in range(4):\n",
    "        df = pd.read_csv(f\"../MIND_large/csv/tensorflow_dataset_chunk{i}.csv\", index_col=0)\n",
    "        full = pd.concat([full, df])\n",
    "    return full\n",
    "full_tf = create_big_boy()\n",
    "news_text = pd.read_csv('../MIND_large/csv/news_cluster_labels.csv')\n",
    "all_ratings2 = full_tf.groupby('user_id')['news_id'].apply(list).reset_index()\n",
    "scores = full_tf.groupby('user_id')['score'].apply(list).reset_index()\n",
    "all_ratings2['scores'] = scores['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create our matrices while respecting the need for hash maps for the ALS algorithm \n",
    "We need a map of 'item' index and user indexes to optimize V\n",
    "We need a map of 'user' index and item indexes to optimize U \n",
    "When we create the item cluster matrix we are taking the dataset as well as the number of users or items depending on our clustering method\n",
    "\n",
    "So generally speaking for our map building, \n",
    "For item index and the user indices, we initialize a map of all 'items' as indices, and since were quantifying users by their index after grouping them together we can just append them to a list at the item index whenever they show up\n",
    "For user index and item indices, in the for loop all we have to do is check if the user is there and if not we add them but if so we add their item index, which for item clustering is just their cluster, under user clustering with normal items we would have to find each items index and then after having each items index we would need to see when a cluster rates something we'd attach that index to their list, and then add that cluster to the item index list in the other table, so tbh lots of hash maps wow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matrix_modules\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [oid sha256:a9c27e2669c351fda4471b981356d4574c81cebb3455ccd683990cb7a3edc305, size 2030881682, oid sha256:1e6d5c7fd8579951d88fb601f7d41f397fbea17de0345aaa84f0a3d68c04ed70, size 1961173597]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'user_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ratings, news, users \u001b[38;5;241m=\u001b[39m \u001b[43mmatrix_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/Modelling/matrix_modules.py:15\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(train_split)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(full)\n\u001b[1;32m     14\u001b[0m news \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../MIND_large/csv/news_cluster_labels.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m all_ratings \u001b[38;5;241m=\u001b[39m \u001b[43mfull\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     16\u001b[0m scores \u001b[38;5;241m=\u001b[39m full\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     17\u001b[0m all_ratings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'user_id'"
     ]
    }
   ],
   "source": [
    "ratings, news, users = matrix_modules.load_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
