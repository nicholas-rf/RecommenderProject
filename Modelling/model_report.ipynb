{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System Implementation\n",
    "To ensure a high quality baseline for our own understanding of recommender systems as well as our comparison, we began our evaluation of recommender systems under a variaty of matrix factorization steps for completing the user item matrix and finished by experimenting on how we can introduce hybrid filtering techniques in our recommendations. For our matrix factorization methods we implemented gradient descent, alternating least squares and the generalization of matrix factorization as released in 2010, factorization machines. Factorization machines were implemented mathematically and with a framework that could better compute the necessary elements, fastFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing clustering\n",
    "Under gradient descent and alternating least squares we use a user-item matrix $R$ which via clustering we experimented on. Two different types of the user-item ratings matrix $R$ were used for our baseline, one using user clustering and the other using item clustering. By setting up $R$ in this way we avoided challenges brought on by the binary nature of our data's ratings by increasing the maximum score a rating can achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings calculations for Item and User Clusters\n",
    "The ratings for user $u_i$ in an item cluster $IC$ is denoted as $IC_i$ and is calculated as the sum of all interactions that user $u_i$ had with various items in the cluster. Mathematically, it can be expressed as:\n",
    "$$\n",
    "IC_{i} = \\sum_{j=1}^{n} I_{ij}\n",
    "$$\n",
    "The ratings for item $v_{j}$ in a user cluster $UC$ is denoted as $UC_{j}$ and is calculated as the sum of all interactions that users in the cluster had with item $v_{j}$. Mathematically, it can be expressed as:\n",
    "$$\n",
    "UC_{j} = \\sum_{i=1}^{m} I_{ij}\n",
    "$$\n",
    "\n",
    "Under both user and item clustering:\n",
    "- $n$ or $m$ represent the total number of items in a cluster,\n",
    "- $I_{ij}$ is a binary indicator defined as follows $ I_{ij} = \\begin{cases} 1, & \\text{if user } u_{i} \\text{ has interacted with item } v_{j} \\\\ \n",
    "0, & \\text{otherwise} \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking\n",
    "Ranking is the step where we take a sparse set of ratings, $R$, and try to predict what unknown ratings are, $\\hat{R}$.\n",
    "\n",
    "For matrix factorization techniques, predicting ratings is formulated as the following non-convex optimization problem which seeks to minimize least squared error and use regularization to avoid overfitting:\n",
    "\n",
    "$$\\min_{U,V} \\sum_{r_{ij} \\text{observed}}{(r_{ij}-u_{i}^Tv_{j})^2} + \\lambda(\\sum_{i}\\|u_i\\|^2 + \\sum_{j}\\|v_j\\|^2)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features \n",
    "To utilize complex features we experimented with appending user and item features to their corresponding latent factor matrices, $U$ and $V$. Under ALS respective features were concatenated to the bottom of $U$ and $V$ whereas in SGD the features were concatenated to the right side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "### Introduction\n",
    "Under gradient descent, we create partial derivatives of the aformentioned function with respect to $u_i$ and $v_j$ to acquire update functions for each vector. Using these update functions each vector in U and V at indices corresponding to observed ratings are updated.\n",
    "\n",
    "The update formulae for $u_i$ and $v_j$ with $i$ and $j$ corresponding to the row and column of an observed rating, are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u_i^{\\text{new}} &= u_i + 2\\alpha (r_{ij} -  v_j u_i^T)\\cdot v_j - 2\\alpha\\lambda u_i\\\\\n",
    "v_j^{\\text{new}} &= v_j + 2\\alpha (r_{ij} -  v_j u_i^T)\\cdot u_i - 2\\alpha\\lambda v_j\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Our implementation of gradient descent for ratings matrix predictions is simple; iterate over the indices of all observed ratings updating corresponding rows in U and V at each rating. During this process we track the updates so that we can stop the training loop early if the algorithm starts to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares\n",
    "### Introduction\n",
    "Alternating Least Squares, or ALS, is another method of matrix factorization similar to gradient descent with one key difference; instead of updating by vector, entire matrices $U$ and $V$ alternate as a fixed variable while optimizing the other. Our implementation of ALS was based off of lecture 14 from CME 323: Distributed Algorithms and Optimization, Spring 2015 from Stanford. Under our implementation of ALS from Stanford the user and item matrices $U$ and $V$ both are of dimension $k$ x $n$ and $k$ x $m$ respectively. The complete ratings matrix $R$ is thus estimated via $\\hat{R} = U^TV$. \n",
    "\n",
    "Compared to gradient descent, ALS is faster and requires less iterations. By fixing $U$ we obtain a convex function of $V$ and vice versa. Therefore in ALS we fix and optimize opposite matrices until convergence. Below is the general algorithm as described in the Stanford materials:\n",
    "\n",
    "* Initialize $k$ x $n$ and $k$ x $m$ matrices $U$ and $V$\n",
    "* Repeat the the following until convergence\n",
    "    * For all column vectors $i = 1,... , n$    \n",
    "    $$ u_i = (\\sum_{r_{ij}\\in r_{i *}}{v_jv_j^T + \\lambda I_k})^{-1} \\sum_{r_{ij}\\in r_{i *}}{r_{ij}v_{j}}$$\n",
    "\n",
    "    * For all column vectors $j = 1,... , m$\n",
    "    $$ v_j = (\\sum_{r_{ij}\\in r_{* j}}{u_iu_i^T + \\lambda I_k})^{-1} \\sum_{r_{ij}\\in r_{* j}}{r_{ij}u_{i}}$$\n",
    "\n",
    "To break it down into pieces:\n",
    "* $\\sum_{{r_{ij}\\in r_{i *}}} v_jv_j^T$ and $\\sum_{r_{ij}\\in r_{* j}} u_iu_i^T$ represent the sum of column vectors multiplied by their transpose where the vectors are determined by either the column vectors correspond to items that user $u_i$ has rated in $V$ or the column vectors correspond to the users in $U$ that have rated item $v_j$.\n",
    "* $\\lambda I_k$ represents the addition of a regularization term $\\lambda$ to avoid overfitting.\n",
    "* $\\sum_{r_{ij}\\in r_{i *}}{r_{ij}v_{j}}$ and $\\sum_{r_{ij}\\in r_{* j}}{r_{ij}u_{i}}$ represent the scaling of each column feature vector by a rating with indexing handled in the same way as $\\sum v_jv_j^T$ and $\\sum u_iu_i^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implementation \n",
    "ALS implementation starts during the creation of the matrix $R$. Since ALS requires us to subset $V$ and $U$ for columns that correspond to items a user has rated or users that have rated an item we used several hash maps to store these indices. Hash maps were created during matrix initialization to take advantage of already iterating over items that a user has rated. With the matrix created and our maps initialized, we created $U$ and $V$ as random matrices with numbers drawn from a uniform distribution. For the optimization steps we found that $\\sum_{r_{ij}\\in r_{i *}}{v_jv_j^T}$ and $\\sum_{r_{ij}\\in r_{* j}}{u_iu_i^T}$ are the same as $V_jV_j^T$ and $U_iU_i^T$ with $U_i$, $U_j$, $V_j$ and $V_j^T$ being the subsets of $U$ and $V$ corresponding to observed ratings. However, this same process did not apply to $\\sum_{r_{ij}\\in r_{i *}}{r_{ij}v_{j}}$ and $\\sum_{r_{ij}\\in r_{* j}}{r_{ij}u_{i}}$, instead we found that we could multiply the observed ratings as a row vector by $V_j^T$ or $U_i^T$ and get the same result as taking the sum. Additionally to introduce regularization we added $\\lambda$ multiplied by the $k$ x $k$ identity matrix to $U_iU_i^T$ and $V_iV_i^T$ while the matrices were being updated. \n",
    "\n",
    "Our final update functions for our matrices thus looked like:\n",
    "\n",
    "$ u_i = ({V_jV_j^T + \\lambda I_k})^{-1} {R_{i*}V_{j}^T}$\n",
    "\n",
    "\n",
    "$ v_j = ({U_iU_i^T + \\lambda I_k})^{-1} {R_{*j}U_{i}^T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorization Machines\n",
    "### Introduction\n",
    "Introduced in 2010, factorization machines offered a generalization of matrix factorization methods. Factorization machines capture all single and pairwise interactions between variables with a closed model equation computable in linear time which is very effective. This is advantagous as it allows for the usage of stochastic gradient descent to learn model parameters. \n",
    "\n",
    "Factorization machines utilize high dimensional feature vectors along with a feature matrix denoted as $V$. For our implementation we implemented a factorization model of degree 2, which per the introductory paper on factorization machines __source__ has the following equation: \n",
    "\n",
    "$$ \\hat{y}(x) := w_0 + \\sum_{i=1}^{n}{w_i x_i} + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} \\langle \\bold{v}_i, \\bold{v}_j\\rangle x_i x_j$$\n",
    "\n",
    "The model parameters that are estimated include $w_0$, $w$ and $V$, where $w_0$ represents global bias, $w$ represents the weights of all possible features in a feature vector $x$ and $V$ is an $n$ x $k$ feature matrix. Pairwise interactions are modeled by $\\langle \\bold{v}_i, \\bold{v}_j\\rangle.$\n",
    "\n",
    "A row within the feature matrix $V$ is defined as $v_i$ which describes the i-th feature with $k$ factors where $k$ represents the dimensionality of the factorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Factorization Machines\n",
    "As mentioned earlier, model parameters $w_0$, $w$ and $V$ can all be learned via gradient descent methods on a variety of losses. As a result, we utilized stochastic gradient descent to optimize and tune our model parameters with our data. Below is the gradient vector of the function $\\hat{y}$ for the estimated model parameters.\n",
    "$$\\frac{\\partial}{\\partial\\theta}\\hat{y}(x) = \\begin{cases} 1, & \\text{if } \\theta \\text{ is } w_0 \\\\ x_i, & \\text{if } \\theta \\text{ is } w_i \\\\ x_i\\sum_{j=1}^{n}{v_{j,f}}x_j - v_{i,f}x_i^2, & \\text{if } \\theta \\text{ is } v_{i,f} \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, to stay consistent in our judgement of our baseline models, we focused on minimizing residuals under the squared loss function, $(y - \\hat{y})^2$, which is shown below: \n",
    "\n",
    " $$(y -  (w_0 + \\sum_{i=1}^{n}{w_i x_i} + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} \\langle \\bold{v}_i, \\bold{v}_j\\rangle x_i x_j))^2$$\n",
    "\n",
    "Given the gradient vector and standard loss we create the following update formulae: \n",
    "\n",
    "$w_0^{new} = w_0 + \\alpha2(y - \\hat{y})$\n",
    "\n",
    "$w_i^{new} = w_i + \\alpha2(y - \\hat{y}) * x_i $\n",
    "\n",
    "$v_{i,f}^{new} = v_{i,f} + \\alpha2(y - \\hat{y}) * x_i\\sum_{j=1}^{n}{v_{j,f}}x_j - v_{i,f}x_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations\n",
    "\n",
    "We implemented two versions of factorization machines, one by hand using the gradients as described above and another using a factorization machine framework fastFM. We utilized our own implementation on a small subset of the data to ensure we have an understanding of how gradient descent can be applied to different optimization problems. The fastFM implementation was used to test out the effectiveness of a factorization machine on our entire dataset. \n",
    "\n",
    "#### Our Own Implementation\n",
    "Factorization machines facilitate the usage of high dimensional feature vectors meaning proper management of large data becomes a priority. Our solution to the issue of memory was a sparse matrix that was created from the Tensorflow compatible dataset to hold feature vectors efficiently in memory. Feature vectors included information about the user, their interaction, rating, previous interactions, median time of day of interactions, item features and personal taste. Model parameters $V$, $w_0$ and $w$ were initialized randomly with samples from a uniform distribution. Parameters were updated at different cadences as every feature vector contained multiple $w_i$ and $v_{i,f}$, meaning $w_0$ was updated once every feature vector and $w_i$ and $v_{i,f}$ were updated for every instance of a feature within a single feature vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matrix_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hat(w_0, x, w, features_matrix):\n",
    "    \"\"\" \n",
    "    Calculates the predicted error for the given parameters: w_0, x, w and the subset of V corresponding to the features present in the feature vector.\n",
    "\n",
    "    Args:\n",
    "        w_0 (int) : w_0 represents the global bias term added at the beggining of the y_hat calculation.\n",
    "        x (dict) : The feature vector corresponding to a particular row index. x has two keys, indices and scores, which signify the indices in V and w that have scores and the corresponding scores.\n",
    "        w (np.ndarray) : w represents the vector of feature weights and is a 1 x num_features dimensional row vector.\n",
    "        features_matrix (np.ndarray) : features_matrix represents the subsetted matrix of V corresponding to the indices containing values in x.\n",
    "    \n",
    "    Returns:\n",
    "        y_hat (int) : A predicted score for the feature vector.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the feature indices and their corresponding scores from the feature vector x.\n",
    "    feature_indices = x[\"indices\"]\n",
    "    scores = x[\"scores\"]\n",
    "\n",
    "    # Get the \\sum{i=1}^{n}{w_ix_i} term.\n",
    "    scaled_feature_weights = w[feature_indices] * scores\n",
    "    \n",
    "    # Initialize a total for the summation of inner product of pairwise rows in V with scores.\n",
    "    total = 0\n",
    "\n",
    "    # Get the number of rows in the subset of the original features matrix for looping.\n",
    "    rows, _ = features_matrix.shape\n",
    "\n",
    "    # Loop through all pairwise groups of rows finding their innner product and multiplying by their scores, summing the whole thing.\n",
    "    for row_1 in range(rows):\n",
    "        for row_2 in range(rows):\n",
    "            # Check to see if one row is the same as another as we wish to avoid that case. \n",
    "            if row_1 == row_2:\n",
    "                pass\n",
    "            else:\n",
    "                # Add the full calculation to total.\n",
    "                # Subsetting scores in this way works because the number of scored items directly corresponds to the number of rows in the subset of our feature matrix V.\n",
    "                total += scores[row_1] * x[row_2] * np.inner(features_matrix[row_1, :], features_matrix[row_2, :])\n",
    "                \n",
    "    # Return the linear combination of what we calculate to get the score prediction.\n",
    "    return w_0 + total + scaled_feature_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w_0(w_0, err, alpha):\n",
    "    \"\"\"\n",
    "    Update the global bias term using the error and the learning rate alpha.\n",
    "    \"\"\"\n",
    "    return w_0 + 2 * alpha * err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w_i(x_i_score, w_i, err, alpha):\n",
    "    \"\"\"\n",
    "    Update an index in the feature vector w using the the score in the feature vector, its weight in w, the error and the learning rate alpha.\n",
    "\n",
    "    Args:\n",
    "        x_i_score (int) : The score in the feature vector x corresponding to the feature with weight w_i.\n",
    "        w_i (int) : The weight of the feature in w.\n",
    "        err (int) : The error calculated by finding y - y_hat.\n",
    "        alpha (int) : The learning rate that was chosen at model creation. \n",
    "\n",
    "    Returns:\n",
    "        w_i (int) : The updated model weight at the ith index in w.\n",
    "    \"\"\"\n",
    "    return w_i + 2*alpha*err*x_i_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_v_ij(x, v_ij, subset, row_i, err, alpha):\n",
    "    \"\"\"\n",
    "    Updates the ith vector in the feature matrix V.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray) : The row vector containing scores from a feature vector. \n",
    "        v_ij (np.ndarray) : A row vector from the feature matrix V. \n",
    "        subset (np.ndarray) : The subset of V corresponding to the indices of interacted with features.\n",
    "        row_i (int) : The row index in the subset that is being updated.\n",
    "        err (int) : The error calculated by finding y - y_hat.\n",
    "        alpha (int) : The learning rate that was chosen at model creation. \n",
    "    \n",
    "    Returns:\n",
    "        Updated v_ij, the updated row vector in V.\n",
    "    \"\"\"\n",
    "    # Get the number of rows in the subset to determine number of looping iterations.\n",
    "    rows, k = subset.shape\n",
    "\n",
    "    # Initialize a total to keep track of the sum.\n",
    "    total = np.zeros((1, k))\n",
    "\n",
    "    # Start looping over the rows of the subset not corresponding to the row determined by row_i.\n",
    "    for j in range(rows):\n",
    "        if j == row_i:\n",
    "            pass\n",
    "        else:\n",
    "            total += subset[j, :] * x[j] - subset[row_i, :] * x[row_i]**2\n",
    "\n",
    "    return v_ij + 2 * alpha * err * x[row_i] * total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorization_machine(feature_vectors, k, num_features, alpha):\n",
    "    \"\"\"\n",
    "    Takes in a set of feature vectors, the desired number of latent factors and the learning rate and then\n",
    "    performs gradient descent in the vain of a factorization machine to train model parameters w_0, w and V.\n",
    "\n",
    "    Args:\n",
    "        feature_vectors (dict) : The sparse representation of feature vectors as a dictionary with keys 'indices' and 'scores'\n",
    "        k (int) : The desired number of latent factors. \n",
    "        num_features (int) : The maximum index in the row vectors which gets used in the creation of V.\n",
    "        alpha (int) : The learning rate for the gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "        w_0, w and V which after running will be trained model weights that can be used on new observations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize w_0.\n",
    "    w_0 = 1\n",
    "    \n",
    "    # Initialize w.\n",
    "    w = np.random.uniform(0, 1, size=num_features).reshape((1, num_features))\n",
    "    w_old = np.zeros_like(w)\n",
    "\n",
    "    # Initialize V\n",
    "    V = np.random.uniform(0, 1, size=num_features*k).reshape((num_features, k))\n",
    "    V_old = np.zeros_like(V)\n",
    "\n",
    "    # Iterate through all rows provided by the feature feature vectors argument.\n",
    "    for row in range(len(feature_vectors)):\n",
    "\n",
    "        # Get the indices of the features that are used and scores.\n",
    "        indices = feature_vectors[row][\"indices\"]\n",
    "        scores = feature_vectors[row][\"scores\"]\n",
    "        rating = feature_vectors[row][\"rating\"]\n",
    "        # Subset V for the rows corresponding to rated feature indices.\n",
    "        V_subset = V[indices, :]\n",
    "\n",
    "        # Calculate y_hat.\n",
    "        rating_estimate = y_hat(w_0, row, w, V_subset)\n",
    "        error =  rating - rating_estimate\n",
    "\n",
    "        for index in range(len(scores)):\n",
    "            # We first update the ith weight in w.\n",
    "            w[:, indices[index]] = update_w_i(scores[index], w[:, indices[index]], error, alpha)\n",
    "\n",
    "            # We then update the ith row of the feature matrix V.\n",
    "            V[indices[index], :] = update_v_ij(scores, V[indices[index], :], V_subset, index, error, alpha)\n",
    "\n",
    "        w_0 = update_w_0(w_0, error, alpha)\n",
    "    \n",
    "    return w_0, w, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Starting off with testing we evaluate the performance of stochastic gradient descent, alternating least squares, and factorization machines. Factorization machines were implemented with an existing framework ___ in order to save time on fine tuning linear algebra for compute efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably the other most important part of a recommender is the part that makes recommendations with the predicted ratings. The other largest sub-system of any recommender system is one that takes action on predicted ratings to make recommendations. What's considered a 'good' recommendation differs greatly depending on the goals in place for the recommender system as a whole. For example, if there are business incentives to promote articles of a certain category then 'good' recommendations are ones that a user will interact with and that further business goals. To ensure that our evaluation was comprehensive, we created a basic system to make recommendations on which we improved upon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics of Success\n",
    "Recommendations can be evaluated by several success metrics: novelty, coverage and serendipity to name a few. Novelty is a measure of the newness of a recommendation. Coverage is a measure of how much of the catalogue is represented in recommendations. Serendipity measures both the newness of recommendations and how exciting they are. An example of a serendipitous recommendation would be if a user generally reads stories about sports in the united states and is then recommended a story about a lesser known sport from a different country they were unaware of. Improving and evaluating the serendipity of recommendations is both difficult and not agreed upon, therefore we chose the following ____ (research serendipity calculations) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of metrics\n",
    "\n",
    "* To understand the coverage of our recommendations, we took the summation of all recommendations made and then determined the portion of the overall catalogue that was comprised of the recommendations. \n",
    "* To calculate the novelty of our recommendations, we utilized a vector similarity of item features to determine how similar recommended items were to those that were previously rated in addition to the popularity of articles.\n",
    "* To calculate serendipity, we ___  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering Edge Cases\n",
    "In addition to improving the quality of recommendations in the directions that we deemed 'good' we also had to consider a couple different edge cases to improve the robustness of our systems. One such edge case that we explored was how we might deal with users that have ranked every available item, users that haven't ranked any items, or ones without many features to go off of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Implementation of Recommender Step\n",
    "For matrix factorization methods like gradient descent and alternating least squares, the most simple form of retrieval is to look at the highest n ratings present in a row of $\\hat{R}$. This way of providing recommendations while simple is prone to providing recommendations of previously viewed items to users which immediately decreases its quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "Improving the quality of recommendations from ratings created by matrix factorization methods like ALS and SGD is a varied process. Since there's no 'best' way to recommend items we focused on improving recommendations in a few contexts: improving recommendations for a specific business goal, making recommendations more personal and adding another step to make intra-cluster recommendations. \n",
    "\n",
    "As mentioned previously, one of our first goals for improving recommendations was to avoid recommending items that a user has already rated. The way we implemented this was by utilizing the hash maps that are made during matrix creation to easily subset out viewed items from the set of recommendations prior to sorting it for the highest ratings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Recommendations : Utilizing Item Features\n",
    "\n",
    "Current thoughts is leaning towards some weighted sum score for each item, like a feature similarity + dissimilarity + predicted ratings thingaling \n",
    "\n",
    "One way that we improved the quality of recommendations was to utilize both latent factors and given features from users and items to  \n",
    "We further experimented on improving recommendations by utilizing the existing features alongside latent factors generated during matrix factorization for both users and items. Features were placed inside of several KNN models where a vector similarity could be used on highly rated and seen items to introduce the usage of features in similarity. User features were used to get the similar users and find recommendations that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and load functions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matrix_modules\n",
    "\n",
    "def calculate_coverage(items_recommended, items_present):\n",
    "    \"\"\" \n",
    "    Calculates the coverage of recommendations made.\n",
    "\n",
    "    Args:\n",
    "        items_recommended (int) : The total number of items that are recommended to users.\n",
    "        items_present (int) : The total number of items present in the catalog.\n",
    "\n",
    "    Returns:\n",
    "        coverage (float) : Returns the percent coverage of the catalog.\n",
    "    \"\"\"\n",
    "\n",
    "    if items_recommended > items_present:\n",
    "        return 1   \n",
    "    else:\n",
    "        return items_recommended / items_present\n",
    "\n",
    "def calculate_novelty(recommendations, item_features, max_popularity):\n",
    "    \"\"\"\n",
    "    Calculates the novelty of recommendations. Since novelty can be a measure of how popular items are \n",
    "    we calculate novelty by 1 - popularity(i) / max popularity\n",
    "    \"\"\"\n",
    "    # Here we would just want to subset the recommendations made in the news features for their popularity and do calculations off of that\n",
    "    # Iterate over all recommendations that are being made\n",
    "    novelty_scores = []\n",
    "    for recommendation in recommendations:\n",
    "\n",
    "        # Recommendation is an index so we subset the item features for that index and get the popularity there \n",
    "        novelty_scores.append(1 - (item_features.loc[recommendation, 'popularity'] / max_popularity))\n",
    "\n",
    "    return novelty_scores\n",
    "\n",
    "def filer_recommendations(sorted_indices, viewed_items):\n",
    "    \"\"\"\n",
    "    Filters the recommendations to exclude those that have already been rated by the user\n",
    "    \"\"\"\n",
    "    # Create an empty list to populate with recommendable indices.\n",
    "    recommendable_items = []\n",
    "\n",
    "    # Sort through all indices that are recommendable.\n",
    "    for index in sorted_indices:\n",
    "\n",
    "        # If the index corresponds with one that has not already been viewed, add it to the list of recommendable items.\n",
    "        if index not in viewed_items:\n",
    "            recommendable_items.append(index)\n",
    "    \n",
    "    # If the recommendable items list is empty, return the sorted indices. \n",
    "    if not recommendable_items:\n",
    "        recommendable_items = sorted_indices\n",
    "        print(\"User has interacted with all possible items\")        \n",
    "    \n",
    "    # If the recommendable items list is not empty, return it.\n",
    "    return recommendable_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>popularity</th>\n",
       "      <th>autos</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>finance</th>\n",
       "      <th>foodanddrink</th>\n",
       "      <th>games</th>\n",
       "      <th>health</th>\n",
       "      <th>kids</th>\n",
       "      <th>...</th>\n",
       "      <th>voices</th>\n",
       "      <th>watch</th>\n",
       "      <th>weatherfullscreenmaps</th>\n",
       "      <th>weathertopstories</th>\n",
       "      <th>weight-loss</th>\n",
       "      <th>weightloss</th>\n",
       "      <th>wellness</th>\n",
       "      <th>wines</th>\n",
       "      <th>wonder</th>\n",
       "      <th>yearinoffbeatgoodnews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N23144</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N86255</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N93187</td>\n",
       "      <td>3</td>\n",
       "      <td>221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N75236</td>\n",
       "      <td>5</td>\n",
       "      <td>1525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 271 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  news_id  cluster  popularity  autos  entertainment  finance  foodanddrink  \\\n",
       "0  N88753       11          10    0.0            0.0      0.0           0.0   \n",
       "1  N23144       16           5    0.0            0.0      0.0           0.0   \n",
       "2  N86255       22           8    0.0            0.0      0.0           0.0   \n",
       "3  N93187        3         221    0.0            0.0      0.0           0.0   \n",
       "4  N75236        5        1525    0.0            0.0      0.0           0.0   \n",
       "\n",
       "   games  health  kids  ...  voices  watch  weatherfullscreenmaps  \\\n",
       "0    0.0     0.0   0.0  ...     0.0    0.0                    0.0   \n",
       "1    0.0     1.0   0.0  ...     0.0    0.0                    0.0   \n",
       "2    0.0     1.0   0.0  ...     0.0    0.0                    0.0   \n",
       "3    0.0     0.0   0.0  ...     0.0    0.0                    0.0   \n",
       "4    0.0     1.0   0.0  ...     1.0    0.0                    0.0   \n",
       "\n",
       "   weathertopstories  weight-loss  weightloss  wellness  wines  wonder  \\\n",
       "0                0.0          0.0         0.0       0.0    0.0     0.0   \n",
       "1                0.0          0.0         1.0       0.0    0.0     0.0   \n",
       "2                0.0          0.0         0.0       0.0    0.0     0.0   \n",
       "3                0.0          0.0         0.0       0.0    0.0     0.0   \n",
       "4                0.0          0.0         0.0       0.0    0.0     0.0   \n",
       "\n",
       "   yearinoffbeatgoodnews  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "\n",
       "[5 rows x 271 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_features = pd.read_csv(\"../MIND_large/csv/user_features.csv\", index_col=0)\n",
    "item_features = pd.read_csv(\"../MIND_large/csv/item_features.csv\", index_col=0).drop(columns=[\"Unnamed: 0\", \"travel.1\"])\n",
    "\n",
    "    \n",
    "    \n",
    "# argueably it is best to just have the features sorted and in their proper place before any of these steps. \n",
    "    # how do we go about that?\n",
    "    # step 1 is to manage item features\n",
    "    # we want to pass the category columns into an alphabetical sort\n",
    "    # take a subset\n",
    "    # remove the columns\n",
    "    # attach the subset\n",
    "\n",
    "cols = item_features.iloc[:, 4:-1].columns\n",
    "cols.sort_values()\n",
    "item_features = item_features.drop(columns=['title', 'abstract'])\n",
    "\n",
    "item_clusters = item_features['cluster'] \n",
    "item_popularity = item_features['popularity']\n",
    "item_ids = item_features['news_id']\n",
    "\n",
    "\n",
    "user_clusters = user_features['cluster']\n",
    "user_median = user_features['median_time']\n",
    "user_ids = user_features['user_id']\n",
    "\n",
    "item_features = item_features[cols]\n",
    "user_features = user_features[cols]\n",
    "# [item_ids, item_clusters, item_popularity, item_features]\n",
    "# [user_ids, user_clusters, user_median, user_features]\n",
    "item_features = pd.concat([item_ids, item_clusters, item_popularity, item_features], axis=1)\n",
    "user_features = pd.concat([user_ids, user_clusters, user_median, user_features], axis=1)\n",
    "\n",
    "# set(item_features.columns) == set(user_features.columns)  \n",
    "item_features.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>median_time</th>\n",
       "      <th>autos</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>finance</th>\n",
       "      <th>foodanddrink</th>\n",
       "      <th>games</th>\n",
       "      <th>health</th>\n",
       "      <th>kids</th>\n",
       "      <th>...</th>\n",
       "      <th>voices</th>\n",
       "      <th>watch</th>\n",
       "      <th>weatherfullscreenmaps</th>\n",
       "      <th>weathertopstories</th>\n",
       "      <th>weight-loss</th>\n",
       "      <th>weightloss</th>\n",
       "      <th>wellness</th>\n",
       "      <th>wines</th>\n",
       "      <th>wonder</th>\n",
       "      <th>yearinoffbeatgoodnews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U1</td>\n",
       "      <td>37</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U100</td>\n",
       "      <td>43</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U1000</td>\n",
       "      <td>37</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U10000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U100005</td>\n",
       "      <td>35</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 271 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  cluster  median_time     autos  entertainment   finance  \\\n",
       "0       U1       37     0.375000  0.000000       0.236111  0.111111   \n",
       "1     U100       43     0.250000  0.069767       0.046512  0.046512   \n",
       "2    U1000       37     0.375000  0.000000       0.000000  0.000000   \n",
       "3   U10000       16     0.291667  0.011494       0.022989  0.137931   \n",
       "4  U100005       35     0.500000  0.000000       0.000000  0.043011   \n",
       "\n",
       "   foodanddrink  games    health  kids  ...   voices  watch  \\\n",
       "0      0.000000    0.0  0.000000   0.0  ...  0.00000    0.0   \n",
       "1      0.116279    0.0  0.116279   0.0  ...  0.00000    0.0   \n",
       "2      0.111111    0.0  0.000000   0.0  ...  0.00000    0.0   \n",
       "3      0.264368    0.0  0.114943   0.0  ...  0.02381    0.0   \n",
       "4      0.225806    0.0  0.021505   0.0  ...  0.00000    0.0   \n",
       "\n",
       "   weatherfullscreenmaps  weathertopstories  weight-loss  weightloss  \\\n",
       "0                    0.0           0.000000          0.0         0.0   \n",
       "1                    0.0           0.000000          0.0         0.0   \n",
       "2                    0.0           0.000000          0.0         0.0   \n",
       "3                    0.0           0.000000          0.0         0.0   \n",
       "4                    0.0           0.032258          0.0         0.0   \n",
       "\n",
       "   wellness  wines  wonder  yearinoffbeatgoodnews  \n",
       "0  0.000000    0.0     0.0                    0.0  \n",
       "1  0.000000    0.0     0.0                    0.0  \n",
       "2  0.000000    0.0     0.0                    0.0  \n",
       "3  0.011905    0.0     0.0                    0.0  \n",
       "4  0.010753    0.0     0.0                    0.0  \n",
       "\n",
       "[5 rows x 271 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autos</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>finance</th>\n",
       "      <th>foodanddrink</th>\n",
       "      <th>games</th>\n",
       "      <th>health</th>\n",
       "      <th>kids</th>\n",
       "      <th>lifestyle</th>\n",
       "      <th>middleeast</th>\n",
       "      <th>movies</th>\n",
       "      <th>...</th>\n",
       "      <th>voices</th>\n",
       "      <th>watch</th>\n",
       "      <th>weatherfullscreenmaps</th>\n",
       "      <th>weathertopstories</th>\n",
       "      <th>weight-loss</th>\n",
       "      <th>weightloss</th>\n",
       "      <th>wellness</th>\n",
       "      <th>wines</th>\n",
       "      <th>wonder</th>\n",
       "      <th>yearinoffbeatgoodnews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>280.529947</td>\n",
       "      <td>478.449186</td>\n",
       "      <td>854.037346</td>\n",
       "      <td>563.767188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>768.570183</td>\n",
       "      <td>0.041435</td>\n",
       "      <td>1971.312133</td>\n",
       "      <td>0.157738</td>\n",
       "      <td>584.450805</td>\n",
       "      <td>...</td>\n",
       "      <td>89.495757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.240248</td>\n",
       "      <td>215.367745</td>\n",
       "      <td>1.267676</td>\n",
       "      <td>44.872551</td>\n",
       "      <td>118.823446</td>\n",
       "      <td>1.067144</td>\n",
       "      <td>10.189553</td>\n",
       "      <td>0.740745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203.016667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164.895107</td>\n",
       "      <td>111.214722</td>\n",
       "      <td>472.535785</td>\n",
       "      <td>172.812047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>160.230607</td>\n",
       "      <td>0.031577</td>\n",
       "      <td>322.322218</td>\n",
       "      <td>0.141050</td>\n",
       "      <td>208.439166</td>\n",
       "      <td>...</td>\n",
       "      <td>16.353693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.712865</td>\n",
       "      <td>76.589127</td>\n",
       "      <td>0.449970</td>\n",
       "      <td>7.303367</td>\n",
       "      <td>22.956029</td>\n",
       "      <td>0.459909</td>\n",
       "      <td>0.152907</td>\n",
       "      <td>0.329157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>393.411341</td>\n",
       "      <td>341.618276</td>\n",
       "      <td>525.890091</td>\n",
       "      <td>471.424912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>386.617419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>834.994885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>478.892240</td>\n",
       "      <td>...</td>\n",
       "      <td>30.693513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.127834</td>\n",
       "      <td>78.284130</td>\n",
       "      <td>0.693050</td>\n",
       "      <td>32.901661</td>\n",
       "      <td>68.215383</td>\n",
       "      <td>0.340342</td>\n",
       "      <td>0.612286</td>\n",
       "      <td>0.139925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>269.723528</td>\n",
       "      <td>153.644798</td>\n",
       "      <td>589.809823</td>\n",
       "      <td>313.628056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>345.841696</td>\n",
       "      <td>0.283851</td>\n",
       "      <td>740.894392</td>\n",
       "      <td>0.071693</td>\n",
       "      <td>288.562666</td>\n",
       "      <td>...</td>\n",
       "      <td>41.314190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.983306</td>\n",
       "      <td>129.891428</td>\n",
       "      <td>0.185839</td>\n",
       "      <td>16.991315</td>\n",
       "      <td>46.364014</td>\n",
       "      <td>0.444102</td>\n",
       "      <td>2.131246</td>\n",
       "      <td>0.975839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>220.595107</td>\n",
       "      <td>1165.884049</td>\n",
       "      <td>632.930899</td>\n",
       "      <td>537.908384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>777.145966</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>2555.700197</td>\n",
       "      <td>0.192861</td>\n",
       "      <td>1080.955634</td>\n",
       "      <td>...</td>\n",
       "      <td>99.338633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.809189</td>\n",
       "      <td>104.431372</td>\n",
       "      <td>1.834982</td>\n",
       "      <td>73.278086</td>\n",
       "      <td>164.884716</td>\n",
       "      <td>0.419870</td>\n",
       "      <td>1.394562</td>\n",
       "      <td>0.253642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.135747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>203.203139</td>\n",
       "      <td>120.900260</td>\n",
       "      <td>289.851496</td>\n",
       "      <td>184.957352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>177.355252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>394.525310</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>232.042825</td>\n",
       "      <td>...</td>\n",
       "      <td>13.737023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.782351</td>\n",
       "      <td>62.753604</td>\n",
       "      <td>0.418301</td>\n",
       "      <td>13.856284</td>\n",
       "      <td>28.290486</td>\n",
       "      <td>0.265641</td>\n",
       "      <td>0.669048</td>\n",
       "      <td>0.069675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>170.660380</td>\n",
       "      <td>366.554568</td>\n",
       "      <td>566.725729</td>\n",
       "      <td>849.981845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2667.641011</td>\n",
       "      <td>0.118868</td>\n",
       "      <td>2328.303426</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>298.892847</td>\n",
       "      <td>...</td>\n",
       "      <td>185.962193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.389624</td>\n",
       "      <td>96.613304</td>\n",
       "      <td>9.195467</td>\n",
       "      <td>339.488037</td>\n",
       "      <td>420.653530</td>\n",
       "      <td>1.254369</td>\n",
       "      <td>0.474417</td>\n",
       "      <td>0.275415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>678.157365</td>\n",
       "      <td>236.474567</td>\n",
       "      <td>1486.371757</td>\n",
       "      <td>509.905683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>465.449800</td>\n",
       "      <td>0.229411</td>\n",
       "      <td>1031.253442</td>\n",
       "      <td>0.259551</td>\n",
       "      <td>421.258553</td>\n",
       "      <td>...</td>\n",
       "      <td>57.277221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.814283</td>\n",
       "      <td>407.847478</td>\n",
       "      <td>0.684636</td>\n",
       "      <td>21.526922</td>\n",
       "      <td>70.886080</td>\n",
       "      <td>0.609809</td>\n",
       "      <td>2.423396</td>\n",
       "      <td>0.682270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>141.156361</td>\n",
       "      <td>140.359964</td>\n",
       "      <td>554.753185</td>\n",
       "      <td>164.453181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179.437840</td>\n",
       "      <td>0.355187</td>\n",
       "      <td>394.547505</td>\n",
       "      <td>0.098973</td>\n",
       "      <td>234.861745</td>\n",
       "      <td>...</td>\n",
       "      <td>18.019598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.726552</td>\n",
       "      <td>84.261902</td>\n",
       "      <td>0.271682</td>\n",
       "      <td>8.784546</td>\n",
       "      <td>25.942365</td>\n",
       "      <td>0.201321</td>\n",
       "      <td>0.759210</td>\n",
       "      <td>1.161567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>230.862534</td>\n",
       "      <td>245.249635</td>\n",
       "      <td>535.380715</td>\n",
       "      <td>3291.735266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>604.069081</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>898.260733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.351006</td>\n",
       "      <td>...</td>\n",
       "      <td>52.577447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.138800</td>\n",
       "      <td>81.885337</td>\n",
       "      <td>1.362395</td>\n",
       "      <td>47.509931</td>\n",
       "      <td>80.819710</td>\n",
       "      <td>2.653571</td>\n",
       "      <td>0.589477</td>\n",
       "      <td>0.066744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>254.141318</td>\n",
       "      <td>197.863326</td>\n",
       "      <td>558.730441</td>\n",
       "      <td>268.875256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>269.529425</td>\n",
       "      <td>0.154589</td>\n",
       "      <td>629.281109</td>\n",
       "      <td>0.095689</td>\n",
       "      <td>372.803841</td>\n",
       "      <td>...</td>\n",
       "      <td>28.832071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.462998</td>\n",
       "      <td>107.348453</td>\n",
       "      <td>0.522753</td>\n",
       "      <td>19.094999</td>\n",
       "      <td>41.371006</td>\n",
       "      <td>0.221092</td>\n",
       "      <td>1.363369</td>\n",
       "      <td>0.148085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>524.265970</td>\n",
       "      <td>391.853027</td>\n",
       "      <td>951.350792</td>\n",
       "      <td>540.112305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>451.644231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>947.615953</td>\n",
       "      <td>0.014988</td>\n",
       "      <td>489.156259</td>\n",
       "      <td>...</td>\n",
       "      <td>42.999069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.939766</td>\n",
       "      <td>91.296412</td>\n",
       "      <td>0.632600</td>\n",
       "      <td>39.160650</td>\n",
       "      <td>80.531093</td>\n",
       "      <td>0.404663</td>\n",
       "      <td>0.742793</td>\n",
       "      <td>0.195888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>152.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>77.450000</td>\n",
       "      <td>95.309524</td>\n",
       "      <td>80.427778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>208.581973</td>\n",
       "      <td>182.948668</td>\n",
       "      <td>493.633193</td>\n",
       "      <td>213.801550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>242.432604</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>482.406161</td>\n",
       "      <td>0.049848</td>\n",
       "      <td>317.204283</td>\n",
       "      <td>...</td>\n",
       "      <td>25.393270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.802125</td>\n",
       "      <td>99.511633</td>\n",
       "      <td>0.374579</td>\n",
       "      <td>20.324898</td>\n",
       "      <td>36.923219</td>\n",
       "      <td>0.074091</td>\n",
       "      <td>0.677847</td>\n",
       "      <td>0.099206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>67.424242</td>\n",
       "      <td>0.506623</td>\n",
       "      <td>98.060469</td>\n",
       "      <td>0.023016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.826923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>103.639419</td>\n",
       "      <td>311.543546</td>\n",
       "      <td>265.881508</td>\n",
       "      <td>176.083239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.145300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>697.748393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>547.160533</td>\n",
       "      <td>...</td>\n",
       "      <td>30.601326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.315523</td>\n",
       "      <td>55.369163</td>\n",
       "      <td>0.310113</td>\n",
       "      <td>21.020028</td>\n",
       "      <td>51.966183</td>\n",
       "      <td>0.046847</td>\n",
       "      <td>0.090570</td>\n",
       "      <td>0.088697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2734.057270</td>\n",
       "      <td>197.865962</td>\n",
       "      <td>661.781474</td>\n",
       "      <td>440.567829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>291.305948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>641.950763</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>252.929118</td>\n",
       "      <td>...</td>\n",
       "      <td>28.091927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.849241</td>\n",
       "      <td>89.377893</td>\n",
       "      <td>0.475292</td>\n",
       "      <td>20.865824</td>\n",
       "      <td>49.727102</td>\n",
       "      <td>0.451520</td>\n",
       "      <td>0.803857</td>\n",
       "      <td>0.181628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>86.557060</td>\n",
       "      <td>120.505284</td>\n",
       "      <td>322.817349</td>\n",
       "      <td>91.052489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.431531</td>\n",
       "      <td>0.077448</td>\n",
       "      <td>166.030892</td>\n",
       "      <td>0.301788</td>\n",
       "      <td>92.118340</td>\n",
       "      <td>...</td>\n",
       "      <td>8.312098</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>3.707012</td>\n",
       "      <td>107.422930</td>\n",
       "      <td>0.165079</td>\n",
       "      <td>3.048027</td>\n",
       "      <td>9.784138</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.085519</td>\n",
       "      <td>1.065976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>108.775206</td>\n",
       "      <td>1372.583180</td>\n",
       "      <td>324.406351</td>\n",
       "      <td>300.018057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>509.610471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3710.994851</td>\n",
       "      <td>0.048958</td>\n",
       "      <td>485.433641</td>\n",
       "      <td>...</td>\n",
       "      <td>67.803446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.984719</td>\n",
       "      <td>57.716147</td>\n",
       "      <td>1.387855</td>\n",
       "      <td>48.990151</td>\n",
       "      <td>96.759287</td>\n",
       "      <td>0.445260</td>\n",
       "      <td>0.227767</td>\n",
       "      <td>0.169124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>377.268155</td>\n",
       "      <td>203.086714</td>\n",
       "      <td>978.849914</td>\n",
       "      <td>352.418346</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>308.903034</td>\n",
       "      <td>0.024546</td>\n",
       "      <td>681.232792</td>\n",
       "      <td>0.236960</td>\n",
       "      <td>363.091018</td>\n",
       "      <td>...</td>\n",
       "      <td>35.413654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.757062</td>\n",
       "      <td>130.387186</td>\n",
       "      <td>0.327196</td>\n",
       "      <td>19.615885</td>\n",
       "      <td>52.149979</td>\n",
       "      <td>0.416205</td>\n",
       "      <td>1.144594</td>\n",
       "      <td>0.521037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>169.250208</td>\n",
       "      <td>347.294035</td>\n",
       "      <td>406.787971</td>\n",
       "      <td>310.653808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>501.271164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1262.024945</td>\n",
       "      <td>0.092231</td>\n",
       "      <td>478.130370</td>\n",
       "      <td>...</td>\n",
       "      <td>63.411455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.457298</td>\n",
       "      <td>106.904852</td>\n",
       "      <td>1.288203</td>\n",
       "      <td>31.710735</td>\n",
       "      <td>80.171304</td>\n",
       "      <td>0.291404</td>\n",
       "      <td>1.264065</td>\n",
       "      <td>0.462664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.342857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.192857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>67.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>125.182022</td>\n",
       "      <td>85.416872</td>\n",
       "      <td>335.606644</td>\n",
       "      <td>123.085558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>134.127520</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>288.891141</td>\n",
       "      <td>0.278438</td>\n",
       "      <td>179.356979</td>\n",
       "      <td>...</td>\n",
       "      <td>13.695574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.596364</td>\n",
       "      <td>60.772158</td>\n",
       "      <td>0.167571</td>\n",
       "      <td>6.614158</td>\n",
       "      <td>19.814260</td>\n",
       "      <td>0.022266</td>\n",
       "      <td>0.452079</td>\n",
       "      <td>0.234609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>52.083333</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>0.429437</td>\n",
       "      <td>72.580098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.053661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>112.967035</td>\n",
       "      <td>136.367306</td>\n",
       "      <td>237.990793</td>\n",
       "      <td>183.392385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.616353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>617.232805</td>\n",
       "      <td>0.215932</td>\n",
       "      <td>225.139680</td>\n",
       "      <td>...</td>\n",
       "      <td>28.080611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.679653</td>\n",
       "      <td>43.732103</td>\n",
       "      <td>0.429805</td>\n",
       "      <td>15.076468</td>\n",
       "      <td>38.143262</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.433553</td>\n",
       "      <td>0.525191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>273.192778</td>\n",
       "      <td>384.680260</td>\n",
       "      <td>968.660239</td>\n",
       "      <td>1089.236273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>852.984309</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>2087.094138</td>\n",
       "      <td>0.074238</td>\n",
       "      <td>386.288076</td>\n",
       "      <td>...</td>\n",
       "      <td>106.434288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.936151</td>\n",
       "      <td>121.887671</td>\n",
       "      <td>2.208323</td>\n",
       "      <td>62.067171</td>\n",
       "      <td>140.370705</td>\n",
       "      <td>1.401051</td>\n",
       "      <td>1.265898</td>\n",
       "      <td>0.280168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>280.264034</td>\n",
       "      <td>173.824771</td>\n",
       "      <td>930.350230</td>\n",
       "      <td>287.661383</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>288.279306</td>\n",
       "      <td>0.283664</td>\n",
       "      <td>574.918326</td>\n",
       "      <td>0.277612</td>\n",
       "      <td>269.051000</td>\n",
       "      <td>...</td>\n",
       "      <td>29.821696</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>3.801366</td>\n",
       "      <td>121.512131</td>\n",
       "      <td>0.434217</td>\n",
       "      <td>18.289978</td>\n",
       "      <td>41.478155</td>\n",
       "      <td>0.313666</td>\n",
       "      <td>0.391176</td>\n",
       "      <td>0.636863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>750.936765</td>\n",
       "      <td>258.626859</td>\n",
       "      <td>2984.787331</td>\n",
       "      <td>663.736849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>556.182203</td>\n",
       "      <td>0.037334</td>\n",
       "      <td>1095.950772</td>\n",
       "      <td>0.241482</td>\n",
       "      <td>389.545734</td>\n",
       "      <td>...</td>\n",
       "      <td>55.601029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.601187</td>\n",
       "      <td>248.740376</td>\n",
       "      <td>1.471120</td>\n",
       "      <td>33.240382</td>\n",
       "      <td>84.442164</td>\n",
       "      <td>0.955546</td>\n",
       "      <td>0.939039</td>\n",
       "      <td>0.190752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>126.015896</td>\n",
       "      <td>831.363812</td>\n",
       "      <td>313.609457</td>\n",
       "      <td>238.166719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>351.257032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>890.000785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>701.376975</td>\n",
       "      <td>...</td>\n",
       "      <td>32.512234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.264188</td>\n",
       "      <td>63.340500</td>\n",
       "      <td>2.043586</td>\n",
       "      <td>36.754779</td>\n",
       "      <td>80.844634</td>\n",
       "      <td>0.130834</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>106.857050</td>\n",
       "      <td>93.965416</td>\n",
       "      <td>279.384301</td>\n",
       "      <td>121.618120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>157.054562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>245.762260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.171670</td>\n",
       "      <td>...</td>\n",
       "      <td>12.537883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.320301</td>\n",
       "      <td>55.557447</td>\n",
       "      <td>0.341735</td>\n",
       "      <td>33.118379</td>\n",
       "      <td>18.414786</td>\n",
       "      <td>0.069693</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          autos  entertainment      finance  foodanddrink     games  \\\n",
       "0    280.529947     478.449186   854.037346    563.767188  0.000000   \n",
       "1      0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "2      0.000000       1.733333     0.000000      0.000000  0.000000   \n",
       "3    164.895107     111.214722   472.535785    172.812047  0.000000   \n",
       "4    393.411341     341.618276   525.890091    471.424912  0.000000   \n",
       "5    269.723528     153.644798   589.809823    313.628056  0.000000   \n",
       "6      0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "7    220.595107    1165.884049   632.930899    537.908384  0.000000   \n",
       "8      0.055556       0.550000     0.135747      0.000000  0.000000   \n",
       "9      0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "10   203.203139     120.900260   289.851496    184.957352  0.000000   \n",
       "11   170.660380     366.554568   566.725729    849.981845  0.000000   \n",
       "12     0.000000       0.250000     0.000000      0.000000  0.000000   \n",
       "13   678.157365     236.474567  1486.371757    509.905683  0.000000   \n",
       "14     0.000000       0.333333     0.166667      0.000000  0.000000   \n",
       "15   141.156361     140.359964   554.753185    164.453181  0.000000   \n",
       "16   230.862534     245.249635   535.380715   3291.735266  0.000000   \n",
       "17     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "18   254.141318     197.863326   558.730441    268.875256  0.000000   \n",
       "19   524.265970     391.853027   951.350792    540.112305  0.000000   \n",
       "20     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "21     0.250000       0.250000     0.000000      0.200000  0.000000   \n",
       "22     0.250000      77.450000    95.309524     80.427778  0.000000   \n",
       "23     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "24   208.581973     182.948668   493.633193    213.801550  0.000000   \n",
       "25    67.424242       0.506623    98.060469      0.023016  0.000000   \n",
       "26   103.639419     311.543546   265.881508    176.083239  0.000000   \n",
       "27     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "28     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "29  2734.057270     197.865962   661.781474    440.567829  0.000000   \n",
       "30    86.557060     120.505284   322.817349     91.052489  0.000000   \n",
       "31     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "32   108.775206    1372.583180   324.406351    300.018057  0.000000   \n",
       "33     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "34     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "35   377.268155     203.086714   978.849914    352.418346  0.016949   \n",
       "36     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "37   169.250208     347.294035   406.787971    310.653808  0.000000   \n",
       "38     0.142857       1.200000     0.125000      0.250000  0.000000   \n",
       "39   125.182022      85.416872   335.606644    123.085558  0.000000   \n",
       "40    52.083333      66.500000     0.429437     72.580098  0.000000   \n",
       "41     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "42   112.967035     136.367306   237.990793    183.392385  0.000000   \n",
       "43   273.192778     384.680260   968.660239   1089.236273  0.000000   \n",
       "44     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "45   280.264034     173.824771   930.350230    287.661383  0.012346   \n",
       "46   750.936765     258.626859  2984.787331    663.736849  0.000000   \n",
       "47     0.000000       0.000000     0.000000      0.000000  0.000000   \n",
       "48   126.015896     831.363812   313.609457    238.166719  0.000000   \n",
       "49   106.857050      93.965416   279.384301    121.618120  0.000000   \n",
       "\n",
       "         health      kids    lifestyle  middleeast       movies  ...  \\\n",
       "0    768.570183  0.041435  1971.312133    0.157738   584.450805  ...   \n",
       "1      0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "2      0.250000  0.000000   203.016667    0.000000     0.000000  ...   \n",
       "3    160.230607  0.031577   322.322218    0.141050   208.439166  ...   \n",
       "4    386.617419  0.000000   834.994885    0.000000   478.892240  ...   \n",
       "5    345.841696  0.283851   740.894392    0.071693   288.562666  ...   \n",
       "6      0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "7    777.145966  0.023810  2555.700197    0.192861  1080.955634  ...   \n",
       "8      0.000000  0.000000     0.225490    0.000000     0.000000  ...   \n",
       "9      0.000000  0.000000    89.000000    0.000000     0.000000  ...   \n",
       "10   177.355252  0.000000   394.525310    0.050000   232.042825  ...   \n",
       "11  2667.641011  0.118868  2328.303426    0.012500   298.892847  ...   \n",
       "12    11.000000  0.000000     0.071429    0.000000     0.000000  ...   \n",
       "13   465.449800  0.229411  1031.253442    0.259551   421.258553  ...   \n",
       "14     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "15   179.437840  0.355187   394.547505    0.098973   234.861745  ...   \n",
       "16   604.069081  0.033333   898.260733    0.000000   234.351006  ...   \n",
       "17     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "18   269.529425  0.154589   629.281109    0.095689   372.803841  ...   \n",
       "19   451.644231  0.000000   947.615953    0.014988   489.156259  ...   \n",
       "20     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "21     0.250000  0.000000     0.450000    0.000000   152.550000  ...   \n",
       "22     0.000000  0.000000     0.250000    0.000000     0.200000  ...   \n",
       "23     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "24   242.432604  0.005747   482.406161    0.049848   317.204283  ...   \n",
       "25    26.826923  0.000000     0.173289    0.000000     0.266393  ...   \n",
       "26   277.145300  0.000000   697.748393    0.000000   547.160533  ...   \n",
       "27     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "28     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "29   291.305948  0.000000   641.950763    0.011111   252.929118  ...   \n",
       "30    84.431531  0.077448   166.030892    0.301788    92.118340  ...   \n",
       "31     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "32   509.610471  0.000000  3710.994851    0.048958   485.433641  ...   \n",
       "33     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "34     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "35   308.903034  0.024546   681.232792    0.236960   363.091018  ...   \n",
       "36     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "37   501.271164  0.000000  1262.024945    0.092231   478.130370  ...   \n",
       "38    47.342857  0.000000    34.192857    0.000000     0.200000  ...   \n",
       "39   134.127520  0.019608   288.891141    0.278438   179.356979  ...   \n",
       "40     0.214286  0.000000   137.053661    0.000000     0.083333  ...   \n",
       "41     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "42   270.616353  0.000000   617.232805    0.215932   225.139680  ...   \n",
       "43   852.984309  0.043478  2087.094138    0.074238   386.288076  ...   \n",
       "44     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "45   288.279306  0.283664   574.918326    0.277612   269.051000  ...   \n",
       "46   556.182203  0.037334  1095.950772    0.241482   389.545734  ...   \n",
       "47     0.000000  0.000000     0.000000    0.000000     0.000000  ...   \n",
       "48   351.257032  0.000000   890.000785    0.000000   701.376975  ...   \n",
       "49   157.054562  0.000000   245.762260    0.000000   141.171670  ...   \n",
       "\n",
       "        voices     watch  weatherfullscreenmaps  weathertopstories  \\\n",
       "0    89.495757  0.000000              10.240248         215.367745   \n",
       "1     0.000000  0.000000               0.000000           0.000000   \n",
       "2     0.250000  0.000000               0.000000           0.000000   \n",
       "3    16.353693  0.000000               2.712865          76.589127   \n",
       "4    30.693513  0.000000               3.127834          78.284130   \n",
       "5    41.314190  0.000000               5.983306         129.891428   \n",
       "6     0.000000  0.000000               0.000000           0.000000   \n",
       "7    99.338633  0.000000               5.809189         104.431372   \n",
       "8     0.000000  0.000000               0.090909           0.000000   \n",
       "9     0.000000  0.000000               0.000000           0.000000   \n",
       "10   13.737023  0.000000               1.782351          62.753604   \n",
       "11  185.962193  0.000000               3.389624          96.613304   \n",
       "12    0.000000  0.000000               0.000000          21.000000   \n",
       "13   57.277221  0.000000               8.814283         407.847478   \n",
       "14    0.000000  0.000000               0.000000           0.000000   \n",
       "15   18.019598  0.000000               2.726552          84.261902   \n",
       "16   52.577447  0.000000               2.138800          81.885337   \n",
       "17    0.000000  0.000000               0.000000           0.000000   \n",
       "18   28.832071  0.000000               2.462998         107.348453   \n",
       "19   42.999069  0.000000               2.939766          91.296412   \n",
       "20    0.000000  0.000000               0.000000           0.000000   \n",
       "21    0.000000  0.000000               0.000000           0.200000   \n",
       "22    0.000000  0.000000               0.000000           0.000000   \n",
       "23    0.000000  0.000000               0.000000           0.000000   \n",
       "24   25.393270  0.000000               3.802125          99.511633   \n",
       "25    0.000000  0.000000               0.000000           0.100000   \n",
       "26   30.601326  0.000000               2.315523          55.369163   \n",
       "27    0.000000  0.000000               0.000000           0.000000   \n",
       "28    0.000000  0.000000               0.000000           0.000000   \n",
       "29   28.091927  0.000000               1.849241          89.377893   \n",
       "30    8.312098  0.125000               3.707012         107.422930   \n",
       "31    0.000000  0.000000               0.000000           0.000000   \n",
       "32   67.803446  0.000000               3.984719          57.716147   \n",
       "33    0.000000  0.000000               0.000000           0.000000   \n",
       "34    0.000000  0.000000               0.000000           0.000000   \n",
       "35   35.413654  0.000000               4.757062         130.387186   \n",
       "36    0.000000  0.000000               0.000000           0.000000   \n",
       "37   63.411455  0.000000               5.457298         106.904852   \n",
       "38    0.000000  0.000000               0.250000          67.357143   \n",
       "39   13.695574  0.000000               2.596364          60.772158   \n",
       "40    0.000000  0.000000               0.000000           0.000000   \n",
       "41    0.000000  0.000000               0.000000           0.000000   \n",
       "42   28.080611  0.000000               2.679653          43.732103   \n",
       "43  106.434288  0.000000               3.936151         121.887671   \n",
       "44    0.000000  0.000000               0.000000           0.000000   \n",
       "45   29.821696  0.111111               3.801366         121.512131   \n",
       "46   55.601029  0.000000               5.601187         248.740376   \n",
       "47    0.000000  0.000000               0.000000           0.000000   \n",
       "48   32.512234  0.000000               2.264188          63.340500   \n",
       "49   12.537883  0.000000               1.320301          55.557447   \n",
       "\n",
       "    weight-loss  weightloss    wellness     wines     wonder  \\\n",
       "0      1.267676   44.872551  118.823446  1.067144  10.189553   \n",
       "1      0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "2      0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "3      0.449970    7.303367   22.956029  0.459909   0.152907   \n",
       "4      0.693050   32.901661   68.215383  0.340342   0.612286   \n",
       "5      0.185839   16.991315   46.364014  0.444102   2.131246   \n",
       "6      0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "7      1.834982   73.278086  164.884716  0.419870   1.394562   \n",
       "8      0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "9      0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "10     0.418301   13.856284   28.290486  0.265641   0.669048   \n",
       "11     9.195467  339.488037  420.653530  1.254369   0.474417   \n",
       "12     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "13     0.684636   21.526922   70.886080  0.609809   2.423396   \n",
       "14     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "15     0.271682    8.784546   25.942365  0.201321   0.759210   \n",
       "16     1.362395   47.509931   80.819710  2.653571   0.589477   \n",
       "17     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "18     0.522753   19.094999   41.371006  0.221092   1.363369   \n",
       "19     0.632600   39.160650   80.531093  0.404663   0.742793   \n",
       "20     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "21     0.000000    0.000000    0.250000  0.000000   0.000000   \n",
       "22     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "23     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "24     0.374579   20.324898   36.923219  0.074091   0.677847   \n",
       "25     0.000000    0.000000   26.750000  0.000000   0.000000   \n",
       "26     0.310113   21.020028   51.966183  0.046847   0.090570   \n",
       "27     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "28     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "29     0.475292   20.865824   49.727102  0.451520   0.803857   \n",
       "30     0.165079    3.048027    9.784138  0.333333   1.085519   \n",
       "31     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "32     1.387855   48.990151   96.759287  0.445260   0.227767   \n",
       "33     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "34     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "35     0.327196   19.615885   52.149979  0.416205   1.144594   \n",
       "36     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "37     1.288203   31.710735   80.171304  0.291404   1.264065   \n",
       "38     0.000000    0.250000    0.000000  0.000000   0.000000   \n",
       "39     0.167571    6.614158   19.814260  0.022266   0.452079   \n",
       "40     0.000000    0.111111    0.000000  0.000000   0.000000   \n",
       "41     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "42     0.429805   15.076468   38.143262  0.007143   0.433553   \n",
       "43     2.208323   62.067171  140.370705  1.401051   1.265898   \n",
       "44     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "45     0.434217   18.289978   41.478155  0.313666   0.391176   \n",
       "46     1.471120   33.240382   84.442164  0.955546   0.939039   \n",
       "47     0.000000    0.000000    0.000000  0.000000   0.000000   \n",
       "48     2.043586   36.754779   80.844634  0.130834   0.410714   \n",
       "49     0.341735   33.118379   18.414786  0.069693   0.365079   \n",
       "\n",
       "    yearinoffbeatgoodnews  \n",
       "0                0.740745  \n",
       "1                0.000000  \n",
       "2                0.000000  \n",
       "3                0.329157  \n",
       "4                0.139925  \n",
       "5                0.975839  \n",
       "6                0.000000  \n",
       "7                0.253642  \n",
       "8                0.000000  \n",
       "9                0.000000  \n",
       "10               0.069675  \n",
       "11               0.275415  \n",
       "12               0.000000  \n",
       "13               0.682270  \n",
       "14               0.000000  \n",
       "15               1.161567  \n",
       "16               0.066744  \n",
       "17               0.000000  \n",
       "18               0.148085  \n",
       "19               0.195888  \n",
       "20               0.000000  \n",
       "21               0.000000  \n",
       "22               0.000000  \n",
       "23               0.000000  \n",
       "24               0.099206  \n",
       "25               0.000000  \n",
       "26               0.088697  \n",
       "27               0.000000  \n",
       "28               0.000000  \n",
       "29               0.181628  \n",
       "30               1.065976  \n",
       "31               0.000000  \n",
       "32               0.169124  \n",
       "33               0.000000  \n",
       "34               0.000000  \n",
       "35               0.521037  \n",
       "36               0.000000  \n",
       "37               0.462664  \n",
       "38               0.000000  \n",
       "39               0.234609  \n",
       "40               0.000000  \n",
       "41               0.000000  \n",
       "42               0.525191  \n",
       "43               0.280168  \n",
       "44               0.000000  \n",
       "45               0.636863  \n",
       "46               0.190752  \n",
       "47               0.000000  \n",
       "48               0.035714  \n",
       "49               0.044700  \n",
       "\n",
       "[50 rows x 268 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors = 5\n",
    "\n",
    "item_features_knn = NearestNeighbors(n_neighbors=n_neighbors, metric = 'euclidean')\n",
    "user_features_knn = NearestNeighbors(n_neighbors=n_neighbors, metric = 'euclidean')\n",
    "\n",
    "user_features_knn.fit(ordered_clustered_user_features)\n",
    "item_features_knn.fit(clustered_item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&#x27;euclidean&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(metric=&#x27;euclidean&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(metric='euclidean')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_item_features = item_features.drop(columns=['news_id', 'title', 'abstract']).groupby(\"cluster\").agg(sum).reset_index()\n",
    "cluster_popularity = clustered_item_features['popularity']\n",
    "clustered_item_features = clustered_item_features.drop(columns=['popularity', 'cluster'])\n",
    "ordered_user_features = user_features[item_features.iloc[:, 4:-1].columns]\n",
    "median_times = user_features['median_time']\n",
    "user_similarity = pd.concat([median_times, ordered_user_features], axis=1)\n",
    "\n",
    "user_features_knn = NearestNeighbors(n_neighbors=n_neighbors, metric = 'euclidean')\n",
    "item_features_knn = NearestNeighbors(n_neighbors=n_neighbors, metric = 'euclidean')\n",
    "\n",
    "user_features_knn.fit(user_similarity)\n",
    "item_features_knn.fit(clustered_item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the ratings matrix\n",
    "print(\"Loading Dataset\")\n",
    "full_ratings, news, users = matrix_modules.load_dataset_for_matrix()\n",
    "\n",
    "print(\"Getting Ratings Matrix\")\n",
    "R, item_idx, user_idx = matrix_modules.create_item_cluster_mat(full_ratings, news, num_users=len(users), isALS=True, num_clusters=len(news['cluster'].unique()))\n",
    "item_idx = {num : sorted(list(users)) for num, users in item_idx.items()}\n",
    "user_idx = {user_id : sorted(list(ratings)) for user_id, ratings in user_idx.items()}\n",
    "seen = {user_id : set(ratings) for user_id, ratings in user_idx.items()}\n",
    "K = 5\n",
    "I = len(user_idx) \n",
    "M = len(item_idx)\n",
    "U = np.random.uniform(0, 1, size=K*I).reshape((K, I))\n",
    "V = np.random.uniform(0, 1, size=K*M).reshape((K, M))\n",
    "print(\"Starting ALS\")\n",
    "U, V, track_error, track_update = matrix_modules.alternating_least_squares(U, V, R, user_idx, item_idx, max_iterations=10)\n",
    "R_hat = U.T @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize knn models for item features / factors\n",
    "item_features_knn = NearestNeighbors(n_neighbors=n_neighbors, metric = 'euclidean')\n",
    "item_factors_knn =  NearestNeighbors(n_neighbors=n_neighbors, metric = 'euclidean')\n",
    "\n",
    "# Fit the models with the item feature data, drop the cluster column for the inputs into knn\n",
    "item_features_knn.fit(grouped_item_features.drop(columns=\"cluster\"))\n",
    "item_factors_knn.fit(V.T);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the columns of user features to match the grouped item features\n",
    "reordered_user_feature_colulmns = user_features[grouped_item_features.iloc[:, 1:].columns]\n",
    "user_features = pd.concat([user_features.iloc[: , :1], reordered_user_feature_colulmns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_filter(R, user_index, ratings_df, user_features, user_similarity_knn, seen):\n",
    "    \"\"\"\n",
    "    Uses vector similarity to find similar users and their corresponding indices in the ratings matrix\n",
    "    \n",
    "    Args:\n",
    "        user_index (int) : A row index in the ratings matrix corresponding to a user.\n",
    "        ratings_df (pd.DataFrame) : The dataframe of ratings grouped by user id loaded in with load_dataset_for_matrix()\n",
    "        user_similarity_knn (sklearn.neighbors.Knearestneighbors (something)) : The knn model fit on user vectors to pull similar indices from.\n",
    "\n",
    "    Returns:\n",
    "        similar_users (list(int)) : A list containing similar user indices in the ratings matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get the users row from the item features data frame.\n",
    "    user_row = user_features[user_features['user_id'] == ratings_df.iloc[user_index, 0]]\n",
    "\n",
    "    # Get the indices of similar users in the ratings matrix. \n",
    "    _, indices = user_similarity_knn.kneighbors(user_row.drop(columns=['user_id']))\n",
    "\n",
    "    # Initialize an empty set to hold onto item indices.\n",
    "    items_to_rec = set()\n",
    "\n",
    "    # Iterate over all other user indices.\n",
    "    for idx in indices[0][1:]:\n",
    "        \n",
    "        # Get that users sorted items by using the seen dictionary.\n",
    "        sorted_rating_indices = np.argsort(R[idx, list(seen[idx])], axis=0)[::-1]\n",
    "\n",
    "        # For each item, add it to the set of items to recommend.\n",
    "        for item in sorted_rating_indices[:5]:\n",
    "            items_to_rec.add(item)\n",
    "    \n",
    "    # Return items to recommend.\n",
    "    return items_to_rec\n",
    "\n",
    "# need to update the filtering step to ensure that theres at least 5 items, even if already seen\n",
    "\n",
    "def content_filter(user_idx, seen, item_features):\n",
    "    \"\"\"\n",
    "    Introduces content filtering into the recommendation step by finding similar items to the ones already rated by the user.\n",
    "    \"\"\"\n",
    "    items_to_rec = set()\n",
    "    rated = list(seen[user_idx])\n",
    "    for item_idx in rated:\n",
    "        _, indices = item_features_knn.kneighbors(item_features.iloc[item_idx, 1:].to_frame().T)\n",
    "        for index in indices[0][1:]:\n",
    "            items_to_rec.add(index)\n",
    "    \n",
    "    return items_to_rec\n",
    "\n",
    "def calculate_weights(user_feature_vector, item_indices, item_features):\n",
    "    \"\"\"\n",
    "    uses user and item features to calculate the weights to use in filtering techniques\n",
    "    \"\"\"\n",
    "    # Here we take our feature vector which is a dataframe input and then locate its row excluding the user_ID column,\n",
    "    # then we make it into a numpy array and call the reshape method to make it two dimensional, for now it is a 256, 1 shape, then we call its transpose so we get a 1, 256 shape\n",
    "    to_npy = user_feature_vector.iloc[0, 1:].to_numpy().reshape(-1, 1).T\n",
    "    weights = []\n",
    "    for item_index in item_indices:\n",
    "        weight = to_npy @ item_features.iloc[1, 1:].to_numpy().reshape(-1, 1)\n",
    "        print(item_features.iloc[item_index, 1:].to_frame().T)\n",
    "        print(user_feature_vector)\n",
    "        # item_features.iloc[item_idx, 1:].to_frame().T\n",
    "\n",
    "def get_top_N_ratings(row_index, seen=None):\n",
    "    \"\"\"\n",
    "    Gets the top N ratings in a row of the ratings matrix.\n",
    "    \"\"\"\n",
    "\n",
    "def recommend_items(R, user_index, ratings_df, user_features, item_features, user_similarity_knn, seen):\n",
    "    \"\"\" \n",
    "    The main function that combines all other recommendation step stuff.\n",
    "    \"\"\"\n",
    "    collaborative_items = collaborative_filter(R, user_index, ratings_df, user_features, user_similarity_knn, seen)\n",
    "    content_items = content_filter(user_index, seen, item_features)\n",
    "    print(collaborative_items)\n",
    "    print(content_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_items(R, 1, full_ratings, user_features, item_features, user_features_knn, seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_row = user_features[user_features['user_id'] == 'U1']\n",
    "feature_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_wo_pop.iloc[0, :].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_row.iloc[0, 1:].to_numpy().reshape(-1, 1).T @ gif_wo_pop.iloc[1, 1:].to_numpy().reshape(-1, 1)\n",
    "# here we use both 1: for feature row and gif_wo_pop because that allows for us to avoid getting the user id or cluster in our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_weights(feature_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ratings_weights(items, user_features_row, item_features):\n",
    "    \"\"\" \n",
    "    Calculates ratings weights by taking the dot product of the user feature vector with each items feature vector.\n",
    "    Or we could just consider taking the product of the users preference score with the category and then do \n",
    "    something with embeddings maybe?\n",
    "\n",
    "    item_features (rows of itme feature dataframe forming a subset for interacted with items)\n",
    "    user_features_row (the row of user preferences)\n",
    "    \"\"\"\n",
    "    # need to look at the feature table for the item indices and make sure that everything is lining up\n",
    "    # need to also make sure that item indices are all fine for user clustering methods \n",
    "    # then also since we have these latent factors of k dimension we can even consider doing a UMAP reduction of the K factors to\n",
    "    # fit the number of features and then utilize those for recommendations as well maybe? \n",
    "    weights = []\n",
    "    for row_index in range(len(item_features)):\n",
    "        # user_features_row['']\n",
    "        # Basically we just get the items category\n",
    "        item_row = item_features.iloc[row_index, :]\n",
    "        item_row = item_row[item_row != 0] # need to test this\n",
    "        category = item_row['category'].columns#? # need to subset where the value is 1\n",
    "\n",
    "        # Then the weight becomes the users preference for that category\n",
    "        user_features_row[category]\n",
    "        # then in regards to opportunities for embeddings, we could get all embeddings that a user prefers and then do something with that in regards\n",
    "        # to the embeddings of the article? \n",
    "        # And then a popularity weighting?\n",
    "        # maybe we want to include some sort of inverse weighting penalty system where we subtract the vector similarity from the weight?\n",
    "        # weights.append()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_user_ratings(indices, R_hat):\n",
    "    \"\"\"\n",
    "    Takes in a list of indices corresponding to users for similar features.\n",
    "    \"\"\"\n",
    "    \n",
    "    for index in indices:\n",
    "        row = R_hat[index, :]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for similar users we iterate through their highest rated items and add them to the set of filtered recommendations\n",
    "# Or maybe we need to implement a function that utilizes item features and the users preferences to get the highest quality\n",
    "# recommendations when fed a set of indices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we consider how we would combine user features and item features / factors\n",
    "# we could always just take their feature preferences w/o the timestamp and make sure that the order of elements is the same ?\n",
    "# like we could take the vector of preferences u and the vector of item features and then take their dot product and that would \n",
    "# be considered a weight on the users predicted score for the item. \n",
    "# We also might want to dimension reduce or make it so the preferences are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
